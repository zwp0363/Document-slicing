{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6333040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置 ---\n",
    "SOURCE_DOC_FILE = \"./yq2021-0602文娱产业正离文化越来越远.doc\"\n",
    "\n",
    "# Milvus 配置\n",
    "USE_MILVUS_LITE = False  # 您正在使用 Docker 模式\n",
    "MILVUS_HOST = \"192.168.16.138\" \n",
    "MILVUS_PORT = \"19530\"\n",
    "\n",
    "COLLECTION_NAME = \"report_analysis\"\n",
    "ID_FIELD = \"chunk_id\"\n",
    "VECTOR_FIELD = \"vector\"\n",
    "TEXT_FIELD = \"text_content\"\n",
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "MAX_CHUNK_LEN = 600\n",
    "MIN_CHUNK_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8bd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 文档加载 ---\n",
    "def find_libreoffice():\n",
    "    possible_paths = [\n",
    "        r\"C:\\\\Program Files\\\\LibreOffice\\\\program\\\\soffice.exe\",\n",
    "        r\"C:\\\\Program Files (x86)\\\\LibreOffice\\\\program\\soffice.exe\",\n",
    "        r\"D:\\\\LibreOffice\\\\program\\\\soffice.exe\",\n",
    "        \"soffice\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path != \"soffice\" and not os.path.exists(path): continue\n",
    "            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0: return path\n",
    "        except: continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc_as_text(doc_path):\n",
    "    print(\"正在加载文档...\")\n",
    "    libreoffice_path = find_libreoffice()\n",
    "    if not libreoffice_path: return None\n",
    "    \n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    output_dir = os.path.dirname(doc_path)\n",
    "    html_filename = os.path.basename(doc_path).rsplit('.', 1)[0] + '.html'\n",
    "    html_path = os.path.join(output_dir, html_filename)\n",
    "    \n",
    "    if os.path.exists(html_path): os.remove(html_path)\n",
    "    cmd = [libreoffice_path, '--headless', '--convert-to', 'html', '--outdir', output_dir, doc_path]\n",
    "    subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if not os.path.exists(html_path): return None\n",
    "    \n",
    "    content = \"\"\n",
    "    for enc in ['utf-8', 'gb18030', 'gbk']:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding=enc) as f:\n",
    "                content = f.read()\n",
    "                break\n",
    "        except: continue\n",
    "            \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    try: os.remove(html_path)\n",
    "    except: pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 语义切割逻辑 ---\n",
    "\n",
    "def merge_orphan_headers(chunks):\n",
    "    \"\"\"\n",
    "    【核心修复】后处理函数：\n",
    "    检查并合并孤立的标题块，解决重复或断裂问题。\n",
    "    \"\"\"\n",
    "    cleaned_chunks = []\n",
    "    # --- 必须与 split_text_smart 中的 semantic_markers 保持同步 ---\n",
    "    markers = [\n",
    "        \"摘要：\", \n",
    "        \"一，\", \"二，\", \"三，\", \"四，\", \n",
    "        \"毋庸置疑，\", \"究其原因，\", \"更进一步分析，\",\n",
    "        \"进一步来看，\", \"概言之，\", \n",
    "        \"一方面，\", \"另一方面，\", \n",
    "        \"更重要的是，\", \"更何况，\"\n",
    "    ]\n",
    "    \n",
    "    skip_next = False\n",
    "    \n",
    "    for i in range(len(chunks)):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "            \n",
    "        current_chunk = chunks[i].strip()\n",
    "        \n",
    "        # 1. 检查是否是孤立标题 (长度小于15且以标记开头)\n",
    "        is_orphan = (len(current_chunk) < 15) and any(current_chunk.startswith(m) for m in markers)\n",
    "        \n",
    "        # 2. 如果是孤立标题，且后面还有内容\n",
    "        if is_orphan and i + 1 < len(chunks):\n",
    "            next_chunk = chunks[i+1].strip()\n",
    "            \n",
    "            # 情况 A：重复 (下一块已经包含了这个标题)\n",
    "            # 例如 Current=\"一，\", Next=\"一，明星IP化...\"\n",
    "            if next_chunk.startswith(current_chunk):\n",
    "                print(f\"   -> 检测到重复标题 '{current_chunk}'，已自动丢弃孤立块。\")\n",
    "                # 直接忽略当前块，不做任何操作，进入下一次循环处理 next_chunk\n",
    "                continue \n",
    "                \n",
    "            # 情况 B：断裂 (下一块是正文，没有标题)\n",
    "            # 例如 Current=\"一，\", Next=\"明星IP化...\"\n",
    "            else:\n",
    "                print(f\"   -> 检测到断裂标题 '{current_chunk}'，已自动合并到下一块。\")\n",
    "                # 把当前标题拼接到下一块的开头\n",
    "                chunks[i+1] = current_chunk + next_chunk \n",
    "                # 忽略当前块\n",
    "                continue\n",
    "        \n",
    "        # 正常块，或者最后一块，直接保留\n",
    "        cleaned_chunks.append(current_chunk)\n",
    "        \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_smart(text, max_len=MAX_CHUNK_LEN, min_len=MIN_CHUNK_LEN):\n",
    "    print(f\"正在进行智能语义切割...\")\n",
    "    \n",
    "    # 1. 移除废弃内容\n",
    "    text = re.split(r\"〖相关链接：信息〗|〖相关链接：报告〗\", text, 1)[0]\n",
    "    \n",
    "    # 2. 基础清理\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text).strip()\n",
    "    \n",
    "    # 3. 初步切割 (Lookahead)\n",
    "    semantic_markers = [\n",
    "        \"摘要：\", \n",
    "        \"一，\", \"二，\", \"三，\", \"四，\", \n",
    "        \"毋庸置疑，\", \"究其原因，\", \"更进一步分析，\",\n",
    "        \"进一步来看，\", \"概言之，\", \n",
    "        \"一方面，\", \"另一方面，\", \n",
    "        \"更重要的是，\", \"更何况，\"\n",
    "    ]\n",
    "    # 【核心修改】：增加 (?<=[\\n。！？；])\n",
    "    # 含义：在匹配 marker 之前，其前一个字符必须是 换行符(\\n) 或 句号(。) 或 感叹号(！) 或 问号(？) 或 分号(；)\n",
    "    # 注意：Python 的 lookbehind 必须是固定长度，所以这里把标点和换行放在一个字符集里\n",
    "    markers_regex = '|'.join(re.escape(m) for m in semantic_markers)\n",
    "    pattern = f\"(?<=[\\n。！？；])(?=({markers_regex}))\"\n",
    "    \n",
    "    # 注意：re.split 可能会保留空字符串，需要过滤\n",
    "    raw_segments = re.split(pattern, text)\n",
    "    raw_segments = [s.strip() for s in raw_segments if s.strip()]\n",
    "    \n",
    "    initial_chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # 4. 长度控制\n",
    "    for segment in raw_segments:\n",
    "        if len(segment) > max_len:\n",
    "            if current_chunk:\n",
    "                initial_chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            sub_chunks = recursive_split_sentence(segment, max_len)\n",
    "            initial_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            is_semantic_start = any(segment.startswith(m) for m in semantic_markers)\n",
    "            if current_chunk:\n",
    "                if len(current_chunk) + len(segment) < max_len and not is_semantic_start:\n",
    "                    current_chunk += \"\\n\" + segment\n",
    "                else:\n",
    "                    if len(current_chunk) < min_len and not is_semantic_start:\n",
    "                         segment = current_chunk + \"\\n\" + segment\n",
    "                         current_chunk = segment\n",
    "                    else:\n",
    "                        initial_chunks.append(current_chunk)\n",
    "                        current_chunk = segment\n",
    "            else:\n",
    "                current_chunk = segment     \n",
    "    if current_chunk:\n",
    "        initial_chunks.append(current_chunk)\n",
    "    \n",
    "    print(f\"初步切割得到 {len(initial_chunks)} 个块，正在进行孤立标题检查...\")\n",
    "    \n",
    "    # 5. 【关键步骤】执行合并清理\n",
    "    final_chunks = merge_orphan_headers(initial_chunks)\n",
    "    \n",
    "    print(f\"清理完成，最终得到 {len(final_chunks)} 个高质量文本块。\")\n",
    "    return final_chunks\n",
    "\n",
    "def recursive_split_sentence(text, max_len):\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    \n",
    "    # --- 修改点：正则表达式中去掉了分号 '；' ---\n",
    "    # 原代码：splits = re.split(r'([。！？；])', text)\n",
    "    # 新代码：只在 句号(。)、感叹号(！)、问号(？) 处进行切分\n",
    "    splits = re.split(r'([。！？])', text) \n",
    "    \n",
    "    temp_sentence = \"\"\n",
    "    for part in splits:\n",
    "        temp_sentence += part\n",
    "        # --- 修改点：这里的判断条件也要同步去掉分号 ---\n",
    "        if part in [\"。\", \"！\", \"？\"] or len(temp_sentence) > max_len:\n",
    "            if len(current) + len(temp_sentence) > max_len:\n",
    "                if current: chunks.append(current)\n",
    "                current = temp_sentence \n",
    "            else:\n",
    "                current += temp_sentence \n",
    "            temp_sentence = \"\"\n",
    "            \n",
    "    if temp_sentence: \n",
    "        if len(current) + len(temp_sentence) > max_len:\n",
    "            if current: chunks.append(current)\n",
    "            chunks.append(temp_sentence)\n",
    "        else:\n",
    "            current += temp_sentence\n",
    "            \n",
    "    if current: chunks.append(current)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60d0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载文档...\n",
      "正在进行智能语义切割...\n",
      "初步切割得到 19 个块，正在进行孤立标题检查...\n",
      "   -> 检测到重复标题 '二，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '三，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '四，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '毋庸置疑，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '究其原因，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '更进一步分析，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '二，'，已自动丢弃孤立块。\n",
      "   -> 检测到重复标题 '三，'，已自动丢弃孤立块。\n",
      "清理完成，最终得到 11 个高质量文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 584)\n",
      "内容: 〖特别报告〗      政府左右房价？（下）2021年06月02日福卡分析                                  总字数：5694〖...\n",
      "\n",
      ">>> Chunk 1 (长度: 141)\n",
      "内容: 明星效应膨胀式放大，并通过多个出口变现。最典型的便是拟上市公司和明星进行深度绑定，借其人气从资本市场获得巨额回报。2016年就有研究者指出，“明星不靠演戏，而是...\n",
      "\n",
      ">>> Chunk 2 (长度: 175)\n",
      "内容: 二，饭圈邪教化。“饭圈”粉丝构建了一套包括打投组、反黑组、安利站等在内的严密的组织体系，往往乐于将偶像标签化，如“美强惨”，偶像酷似宗教叙事中的受难者，而粉丝则...\n",
      "\n",
      ">>> Chunk 3 (长度: 169)\n",
      "内容: 三，运作金融化。文娱产业俨然成为了一场裹挟着各路资本套路的“买卖”，如近期“倒奶事件”便是在“唯钱是举”的打投应援机制下的产物。“你我本无缘，全靠我花钱”也直接...\n",
      "\n",
      ">>> Chunk 4 (长度: 139)\n",
      "内容: 四，趣味低俗化。一切都能娱乐，一切都可以拿来娱乐，文化趣味也趋向以娱乐为标杆的审丑化、猎奇化、低俗化。各种小鲜肉网剧、打“擦边球”直播、无底线综艺，把受众一网打...\n",
      "\n",
      ">>> Chunk 5 (长度: 174)\n",
      "内容: 毋庸置疑，种种乱象之下，文娱产业正离文化越来越远，“一切公众话语都日渐以娱乐的方式出现，并成为一种文化精神。我们的政治、宗教、新闻、体育、教育和商业都心甘情愿地...\n",
      "\n",
      ">>> Chunk 6 (长度: 596)\n",
      "内容: 究其原因，既因流量至上“博眼球”。互联网时代，流量即资源。而随着互联网用户红利的日渐消弭，在“流量=用户数×用户时长”的规则中，尽可能多地占领用户时长成为一种共...\n",
      "\n",
      ">>> Chunk 7 (长度: 402)\n",
      "内容: 更进一步分析，文娱产业偏离文化的背后也折射出时尚与文化的对冲。从经典文化观来看，尽管对于“文化”尚没有形成一个统一的概念——亨廷顿将文化形容为“一个社会中的价值...\n",
      "\n",
      ">>> Chunk 8 (长度: 116)\n",
      "内容: 二，个性与群体性。经典文化观强调群体性的集成，而时尚尽管面向大众，但本质上崇拜个性的解放，如广告所标榜的是“穿出个性”，而非“穿出整体性”。而一旦人们量大面广地...\n",
      "\n",
      ">>> Chunk 9 (长度: 521)\n",
      "内容: 三，世俗与高雅。精英论者的文化观，认为形式“高雅”的才是文化，但时尚往往迎合的是大众“量大而浅层”的文化需求，这就难免世俗化、最终平庸化。如时尚风向、流行音乐等...\n",
      "\n",
      "正在生成向量...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在连接 Milvus (192.168.16.138)...\n",
      "正在写入磁盘 (Flush)...\n",
      "\n",
      "✅ 任务完成！共存入 11 条数据。\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 主程序 ---\n",
    "def main():\n",
    "    full_text = load_doc_as_text(SOURCE_DOC_FILE)\n",
    "    if not full_text: return\n",
    "\n",
    "    # 1. 执行切割\n",
    "    chunks = split_text_smart(full_text)\n",
    "    \n",
    "    # 2. 预览检查\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"      最终切分效果预览      \")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # --- 保持与切割逻辑一致，以便预览所有类型的关键块 ---\n",
    "    markers = [\n",
    "        \"摘要：\", \n",
    "        \"一，\", \"二，\", \"三，\", \"四，\", \n",
    "        \"毋庸置疑，\", \"究其原因，\", \"更进一步分析，\",\n",
    "        \"进一步来看，\", \"概言之，\", \n",
    "        \"一方面，\", \"另一方面，\", \n",
    "        \"更重要的是，\", \"更何况，\"\n",
    "    ]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        is_marker = any(chunk.startswith(m) for m in markers)\n",
    "        if i < 5 or is_marker:\n",
    "            print(f\"\\n>>> Chunk {i} (长度: {len(chunk)})\")\n",
    "            preview = chunk.replace(\"\\n\", \" \")[:80]\n",
    "            print(f\"内容: {preview}...\")\n",
    "            \n",
    "            # 再次校验\n",
    "            if len(chunk) < 10 and is_marker:\n",
    "                print(\"❌ 错误：仍然存在孤立标题！\")\n",
    "\n",
    "    # 3. 存入 Milvus\n",
    "    print(f\"\\n正在生成向量...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"正在连接 Milvus ({MILVUS_HOST})...\")\n",
    "    if USE_MILVUS_LITE:\n",
    "        connections.connect(\"default\", uri=\"./milvus_demo.db\")\n",
    "    else:\n",
    "        connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    \n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        \n",
    "    fields = [\n",
    "        FieldSchema(name=ID_FIELD, dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=VECTOR_FIELD, dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=TEXT_FIELD, dtype=DataType.VARCHAR, max_length=65535)\n",
    "    ]\n",
    "    collection = Collection(name=COLLECTION_NAME, schema=CollectionSchema(fields))\n",
    "    \n",
    "    collection.insert([embeddings, chunks])\n",
    "    print(\"正在写入磁盘 (Flush)...\")\n",
    "    collection.flush() \n",
    "    \n",
    "    index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "    collection.create_index(VECTOR_FIELD, index_params)\n",
    "    collection.load()\n",
    "    \n",
    "    print(f\"\\n✅ 任务完成！共存入 {collection.num_entities} 条数据。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
