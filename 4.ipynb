{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719f2024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\jupyter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置huggingface镜像源\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import re\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "# 移除sentence_transformers导入\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# 添加text2vec导入\n",
    "# from text2vec import SentenceModel\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0dc3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置区域 ---\n",
    "\n",
    "# 1.1 文档与数据库配置\n",
    "SOURCE_DOC_FILE = \"./yq2021-0112化解当下世界经济衰退的根本之道——基础设施建设.doc\"\n",
    "USE_MILVUS_LITE = False  # Docker 模式设为 False\n",
    "MILVUS_HOST = \"192.168.16.138\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"report_analysis_topic\" # 新集合名称\n",
    "# 修改为text2vec模型\n",
    "# MODEL_NAME = 'text2vec-base-chinese'  # text2vec的中文模型\n",
    "MODEL_NAME = 'shibing624/text2vec-base-chinese'  # 这个是在国内镜像上的中文模型\n",
    "\n",
    "# 1.2 LLM API 配置 (请替换为您自己的服务商)\n",
    "LLM_API_KEY = \"sk-fa13f585000140deabdfa506b25a7f3d\" # 替换这里\n",
    "LLM_BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"      # 替换这里\n",
    "LLM_MODEL = \"qwen-plus\"                       # 替换这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493a0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 文档加载模块 (复用之前的逻辑) ---\n",
    "def find_libreoffice():\n",
    "    possible_paths = [\n",
    "        r\"C:\\Program Files\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"C:\\Program Files (x86)\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"D:\\LibreOffice\\program\\soffice.exe\",\n",
    "        \"soffice\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path != \"soffice\" and not os.path.exists(path): continue\n",
    "            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0: return path\n",
    "        except: continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c880f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc_as_text(doc_path):\n",
    "    print(f\"正在读取原始文档: {doc_path}...\")\n",
    "    libreoffice_path = find_libreoffice()\n",
    "    if not libreoffice_path: \n",
    "        print(\"❌ 未找到 LibreOffice\")\n",
    "        return None\n",
    "    \n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    output_dir = os.path.dirname(doc_path)\n",
    "    html_filename = os.path.basename(doc_path).rsplit('.', 1)[0] + '.html'\n",
    "    html_path = os.path.join(output_dir, html_filename)\n",
    "    \n",
    "    if os.path.exists(html_path): os.remove(html_path)\n",
    "    subprocess.run([libreoffice_path, '--headless', '--convert-to', 'html', '--outdir', output_dir, doc_path], capture_output=True)\n",
    "    \n",
    "    if not os.path.exists(html_path): return None\n",
    "    \n",
    "    content = \"\"\n",
    "    for enc in ['utf-8', 'gb18030', 'gbk']:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding=enc) as f:\n",
    "                content = f.read()\n",
    "                break\n",
    "        except: continue\n",
    "            \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    try: os.remove(html_path)\n",
    "    except: pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc834a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LLM 处理模块 (核心) ---\n",
    "def generate_structured_summary(raw_text):\n",
    "    \"\"\"\n",
    "    调用 LLM API，将原始文本重写为结构化格式\n",
    "    \"\"\"\n",
    "    print(\"\\n正在呼叫 LLM 进行文档重构与总结 (这可能需要几十秒)...\")\n",
    "    \n",
    "    if \"sk-xxxx\" in LLM_API_KEY:\n",
    "        print(\"❌ 错误：请先在脚本中设置有效的 LLM_API_KEY！\")\n",
    "        return None\n",
    "\n",
    "    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)\n",
    "\n",
    "    # 定义系统提示词：强制 AI 输出特定格式\n",
    "    system_prompt = \"\"\"\n",
    "    你是一个专业的文档分析师。请阅读用户提供的文档，并按照以下严格的格式重写和总结内容：\n",
    "    1. 必须使用层级结构。一级标题使用中文数字（如“一、”、“二、”）。\n",
    "    2. 一级标题下必须包含若干二级子主题，二级子主题以冒号结尾（如“明星IP化：”）。\n",
    "    3. 必须保留原文中的关键数据、年份、观点和引用。\n",
    "    4. 不要使用 Markdown 的加粗符号（**），直接输出纯文本。\n",
    "    \n",
    "    输出格式范例：\n",
    "    一、发展背景\n",
    "    时间节点：\n",
    "    2014年发生了...\n",
    "    二、核心乱象\n",
    "    明星IP化：\n",
    "    内容描述...\n",
    "    饭圈化：\n",
    "    内容描述...\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"请处理以下文档内容：\\n\\n{raw_text[:8000]}\"} # 防止超长，截取前8000字符\n",
    "            ],\n",
    "            temperature=0.3 # 较低温度，保证输出格式稳定\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        print(\"✅ LLM 重构完成！\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM 调用失败: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be538b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 结构化切割逻辑 (复用) ---\n",
    "def hierarchical_split(text):\n",
    "    print(\"正在解析结构化文本...\")\n",
    "    chunks = []\n",
    "    # 匹配 \"一、\" 开头的行作为一级分隔符\n",
    "    level1_pattern = r\"(?=(^[一二三四五六七八九十]、))\"\n",
    "    sections = re.split(level1_pattern, text.strip(), flags=re.MULTILINE)\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "\n",
    "    for section in sections:\n",
    "        lines = section.split('\\n')\n",
    "        if not lines: continue\n",
    "        main_title = lines[0].strip()\n",
    "        \n",
    "        current_sub_title = \"概述\" # 默认子标题\n",
    "        current_buffer = []\n",
    "\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            \n",
    "            # 识别二级标题 (以冒号结尾)\n",
    "            if len(line) < 30 and (line.endswith(\"：\") or line.endswith(\":\")):\n",
    "                if current_buffer:\n",
    "                    # 保存上一块\n",
    "                    full_context = f\"【{main_title}】-> {current_sub_title}\\n\" + \"\\n\".join(current_buffer)\n",
    "                    chunks.append(full_context)\n",
    "                \n",
    "                current_sub_title = line.strip(\"：:\")\n",
    "                current_buffer = []\n",
    "            else:\n",
    "                current_buffer.append(line)\n",
    "        \n",
    "        # 保存最后一块\n",
    "        if current_buffer:\n",
    "            full_context = f\"【{main_title}】-> {current_sub_title}\\n\" + \"\\n\".join(current_buffer)\n",
    "            chunks.append(full_context)\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58abf9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取原始文档: ./yq2021-0112化解当下世界经济衰退的根本之道——基础设施建设.doc...\n",
      "\n",
      "正在呼叫 LLM 进行文档重构与总结 (这可能需要几十秒)...\n",
      "✅ LLM 重构完成！\n",
      "\n",
      "--- AI 生成内容预览 ---\n",
      "一、发展背景  \n",
      "时间节点：  \n",
      "2020年新冠疫情席卷全球，全球经济按下“加速键”进入深度衰退。IMF预测2020年全球经济将收缩6%—7.6%，其中美国收缩8.0%，日本收缩5.8%，欧元区收缩10.2%，新兴经济体收缩2.5%。世界银行指出，本轮衰退为近百年来最严重的一次，全球近93%的经济体陷入衰退。叠加“逆全球化”趋势、中美关系恶化及全球经贸链条断裂，世界经济面临二战以来最为严峻的挑战。\n",
      "\n",
      "核心判断：  \n",
      "美联储主席鲍威尔称此次经济衰退“范围和速度尚无现代先例”，过去十年经济增长成果被抹去。疫情防控不力与发展中国家医疗基建短缺形成恶性循环，而发达国家亦难以快速响应基础设施建设需求。在此背景下，报告提出以“基础设施建设”作为破解全球经济困局的根本之道。\n",
      "\n",
      "二、历史经验借鉴  \n",
      "罗斯福新政挽救美国大萧条：  \n",
      "1929—1933年大萧条期间，胡佛政府依赖市场自我调节失败，罗斯福上台后实施新政，启动25万个工程项目，包括修建7.8万座桥梁、105万公里道路、1127公里飞机跑道等。此举不仅安置了800万人就业，还将农村电力供应率从1935年的11%提升至1941年的40%以上。田纳...\n",
      "-----------------------\n",
      "\n",
      "正在解析结构化文本...\n",
      "\n",
      "=== 所有切割结果 ===\n",
      "\n",
      "--- 片段 1 ---\n",
      "【一、发展背景】-> 时间节点\n",
      "2020年新冠疫情席卷全球，全球经济按下“加速键”进入深度衰退。IMF预测2020年全球经济将收缩6%—7.6%，其中美国收缩8.0%，日本收缩5.8%，欧元区收缩10.2%，新兴经济体收缩2.5%。世界银行指出，本轮衰退为近百年来最严重的一次，全球近93%的经济体陷入衰退。叠加“逆全球化”趋势、中美关系恶化及全球经贸链条断裂，世界经济面临二战以来最为严峻的挑战。\n",
      "\n",
      "--- 片段 2 ---\n",
      "【一、发展背景】-> 核心判断\n",
      "美联储主席鲍威尔称此次经济衰退“范围和速度尚无现代先例”，过去十年经济增长成果被抹去。疫情防控不力与发展中国家医疗基建短缺形成恶性循环，而发达国家亦难以快速响应基础设施建设需求。在此背景下，报告提出以“基础设施建设”作为破解全球经济困局的根本之道。\n",
      "\n",
      "--- 片段 3 ---\n",
      "【二、历史经验借鉴】-> 罗斯福新政挽救美国大萧条\n",
      "1929—1933年大萧条期间，胡佛政府依赖市场自我调节失败，罗斯福上台后实施新政，启动25万个工程项目，包括修建7.8万座桥梁、105万公里道路、1127公里飞机跑道等。此举不仅安置了800万人就业，还将农村电力供应率从1935年的11%提升至1941年的40%以上。田纳西水利工程每年产生防灾效益1.4亿美元，接待游客达6500万人次，长期发挥经济效益。\n",
      "\n",
      "--- 片段 4 ---\n",
      "【二、历史经验借鉴】-> 日本基建狂潮成为经济托底神器\n",
      "上世纪60年代，田中角荣提出“日本列岛改造论”，推动全国铁路与高速公路建设。1974年石油危机后，日本延续该战略，通过大规模基建投资摆脱外部冲击。1990年代“失去的二十年”中，日本三次推出基建刺激计划稳定经济。2012年安倍晋三推行“安倍经济学”，本质仍是依靠基建拉动内需、托底增长。\n",
      "\n",
      "--- 片段 5 ---\n",
      "【二、历史经验借鉴】-> “基建狂魔”创造中国式经济奇迹\n",
      "1998年亚洲金融危机时，朱镕基主导发行每年1000亿元特别国债，重点投向铁路、公路、电力、港口等领域。1998—2005年国家电网投入3800亿元完成“村村通电”，带动“家电下乡”，有效刺激农村内需。2008年次贷危机后，“四万亿”计划推动高速公路、机场、水利等项目迅猛发展，助力中国经济在2009年实现“V”型反弹。汽车保有量由2008年6400万辆增至2018年2.4亿辆，私家车从4100万辆升至2.1亿辆，显著拉动制造业复苏。\n",
      "\n",
      "--- 片段 6 ---\n",
      "【三、中国基建引领全球复苏】-> 国内抗疫与经济复苏双轮驱动\n",
      "2020年中国成为全球唯一正增长的主要经济体，除有效防疫外，基建发挥关键作用。武汉火神山、雷神山医院迅速建成，并推进六大医疗基建项目（如蔡甸常福、江夏郑店等地新建三甲医院），各地纷纷建设“小汤山”模式医院，弥补公共卫生短板。\n",
      "\n",
      "--- 片段 7 ---\n",
      "【三、中国基建引领全球复苏】-> 30万亿元长期基建规划启动\n",
      "不同于2008年“四万亿”的应急性质，此次基建投资更具战略性与前瞻性，涵盖传统基建与“新基建”。新基建聚焦5G基站、充电桩、大数据中心、物联网等领域，成为中国抢占未来产业制高点的核心布局。据中国信通院预测，到2025年5G建设累计投资达1.2万亿元，带动产业链上下游及行业应用投资超3.5万亿元，催生超过10万亿元的新经济产值。\n",
      "\n",
      "--- 片段 8 ---\n",
      "【四、中国基建的国际影响】-> 带动全球大宗商品市场回暖\n",
      "中国基建扩张大幅提升对铁矿石、铜、镍、锌等工业金属的需求。数据显示，铁矿石价格上涨逾40%，镍价上涨25%以上，铜价上涨约35%，成为全球工业经济复苏的重要信号。\n",
      "\n",
      "--- 片段 9 ---\n",
      "【四、中国基建的国际影响】-> 输出基建能力改善国际营商环境\n",
      "全球仍有约1/7人口无电可用，南亚、撒哈拉以南非洲地区电网覆盖率极低。中国通过“一带一路”倡议向发展中国家输出电网、铁路、桥梁等成熟基建能力，填补基础设施缺口。例如中土集团承建的埃塞俄比亚至吉布提铁路项目，总投资40亿美元，全面采用中国标准与中国设备，实现轨距、速度、牵引系统等技术统一，形成“中国模式”示范效应。\n",
      "\n",
      "--- 片段 10 ---\n",
      "【五、基建标准输出重塑全球规则】-> 标准先行促进互联互通\n",
      "“互联互通、标准先行”已成为区域合作共识。东盟《互联互通总体规划（2025）》、哈萨克斯坦“光明之路”、老挝“陆联国”战略均强调基础设施联通。中国推动与沿线国家标准对接，消除空白、交叉与冲突，构建统一规范体系。\n",
      "\n",
      "--- 片段 11 ---\n",
      "【五、基建标准输出重塑全球规则】-> 推动人民币国际化与制度性话语权\n",
      "通过亚投行、金砖国家开发银行等多边金融机构提供融资支持，中国不仅解决发展中国家资金难题，也构建友好贸易与结算机制，为人民币国际化奠定基础。同时，中国技术标准、管理方式、组织模式随基建项目输出，逐步升级为国际通行规则，增强在全球经济治理中的话语权。\n",
      "\n",
      "--- 片段 12 ---\n",
      "【六、大国崛起的历史逻辑】-> 历史规律显现\n",
      "- 大航海时代，英国凭借造船与航运基建取代荷兰，成就“日不落帝国”；\n",
      "- 19世纪末，美国超前建设铁路与公路网络，加速工业化进程，主导第二次工业革命，开启“镀金时代”；\n",
      "- 当今中国，既是全球最大传统基建国，又引领信息网络等新型基础设施建设，拥有世界上最发达的基础设施体系。\n",
      "\n",
      "--- 片段 13 ---\n",
      "【六、大国崛起的历史逻辑】-> 战略意义深远\n",
      "中国通过“内外兼修”的大基建战略，既激活国内经济循环，又重塑全球实业格局。在发达国家产业空心化、发展中国家缺乏资金与技术的双重困境下，中国成为全球经济复苏的“新兴旗手”。以“人类命运共同体”理念为指导，中国正从全球游戏参与者转变为规则制定者，用“中国方案”弥合政治分歧，重构安全可靠的全球经贸新秩序。\n",
      "\n",
      "--- 片段 14 ---\n",
      "【六、大国崛起的历史逻辑】-> 结论\n",
      "基础设施建设不仅是应对经济衰退的有效手段，更是大国崛起的战略支点。中国以其强大的基建能力、长远的战略规划和广泛的国际合作，正在以“基建赋能”重塑世界格局，为化解全球经济危机提供根本出路。\n",
      "\n",
      "=== 切割结果结束 ===\n",
      "\n",
      "正在保存切割结果到文件: document_chunks_20251113_164439.txt...\n",
      "✅ 切割结果已成功保存到: document_chunks_20251113_164439.txt\n",
      "正在生成向量并存入 Milvus...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ 任务全流程完成！数据已存入集合 \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m正在生成向量并存入 Milvus...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 修改模型加载方式\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m model = \u001b[43mSentenceModel\u001b[49m(MODEL_NAME)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# text2vec的encode方法参数可能略有不同\u001b[39;00m\n\u001b[32m     44\u001b[39m embeddings = model.encode(chunks)\n",
      "\u001b[31mNameError\u001b[39m: name 'SentenceModel' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 5. 主程序 ---\n",
    "def main():\n",
    "    # 1. 读取\n",
    "    raw_text = load_doc_as_text(SOURCE_DOC_FILE)\n",
    "    if not raw_text: return\n",
    "    \n",
    "    # 2. LLM 生成\n",
    "    structured_text = generate_structured_summary(raw_text)\n",
    "    if not structured_text: return\n",
    "    \n",
    "    # 打印一下 AI 生成的内容前几行，看看效果\n",
    "    print(\"\\n--- AI 生成内容预览 ---\")\n",
    "    print(structured_text[:500] + \"...\\n-----------------------\\n\")\n",
    "\n",
    "    # 3. 切割\n",
    "    chunks = hierarchical_split(structured_text)\n",
    "    \n",
    "    # 添加打印所有切割结果的代码\n",
    "    print(\"\\n=== 所有切割结果 ===\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- 片段 {i} ---\")\n",
    "        print(chunk)\n",
    "    print(\"\\n=== 切割结果结束 ===\")\n",
    "\n",
    "    # 添加：切割完成后立即输出txt文档（带时间戳）\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"document_chunks_{timestamp}.txt\"\n",
    "    print(f\"\\n正在保存切割结果到文件: {output_filename}...\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                f.write(f\"\\n--- 片段 {i} ---\\n\")\n",
    "                f.write(f\"{chunk}\\n\")\n",
    "        print(f\"✅ 切割结果已成功保存到: {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存文件时出错: {e}\")\n",
    "    \n",
    "    # 4. 存入 Milvus\n",
    "    print(f\"正在生成向量并存入 Milvus...\")\n",
    "    # 修改模型加载方式\n",
    "    model = SentenceModel(MODEL_NAME)\n",
    "    # text2vec的encode方法参数可能略有不同\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    # 修改向量维度 - text2vec-base-chinese的维度是384\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=384),  # 修改维度为384\n",
    "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "    ]\n",
    "    collection = Collection(name=COLLECTION_NAME, schema=CollectionSchema(fields))\n",
    "    \n",
    "    collection.insert([embeddings, chunks])\n",
    "    collection.flush()\n",
    "    collection.create_index(\"vector\", {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}})\n",
    "    \n",
    "    print(f\"\\n✅ 任务全流程完成！数据已存入集合 '{COLLECTION_NAME}'。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
