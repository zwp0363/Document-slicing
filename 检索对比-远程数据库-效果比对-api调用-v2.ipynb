{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8d1e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è¯»å–æ–‡ä»¶: d:\\jupyter\\api_retrieval_split_result_v2.xlsx\n",
      "å…± 83 æ¡æ•°æ®ï¼Œä½¿ç”¨ 5 çº¿ç¨‹å¹¶å‘å¤„ç† (å«æ™ºèƒ½å®¹é”™)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMæ™ºèƒ½åˆ†æä¸­:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 30/83 [00:24<00:29,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ è§¦å‘åˆè§„æ‹¦æˆªï¼Œå°è¯•ç¼©å‡æ–‡æœ¬é‡è¯• (1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMæ™ºèƒ½åˆ†æä¸­:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 62/83 [00:50<00:13,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ è§¦å‘åˆè§„æ‹¦æˆªï¼Œå°è¯•ç¼©å‡æ–‡æœ¬é‡è¯• (1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMæ™ºèƒ½åˆ†æä¸­:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 65/83 [00:53<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ è§¦å‘åˆè§„æ‹¦æˆªï¼Œå°è¯•ç¼©å‡æ–‡æœ¬é‡è¯• (1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMæ™ºèƒ½åˆ†æä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 70/83 [00:57<00:08,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ è§¦å‘åˆè§„æ‹¦æˆªï¼Œå°è¯•ç¼©å‡æ–‡æœ¬é‡è¯• (1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMæ™ºèƒ½åˆ†æä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [01:08<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æå®Œæˆï¼Œæ­£åœ¨ä¿å­˜...\n",
      "ğŸ‰ å¤„ç†æˆåŠŸï¼ç»“æœä¿å­˜åœ¨: d:\\jupyter\\final_analysis_comparison_aliyun_v2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import concurrent.futures\n",
    "from openai import OpenAI, BadRequestError, APIConnectionError, RateLimitError\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "\n",
    "# 1. å¤§æ¨¡å‹é…ç½® (é˜¿é‡Œäº‘ DashScope)\n",
    "LLM_API_KEY = \"sk-fa13f585000140deabdfa506b25a7f3d\"\n",
    "LLM_API_BASE = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "MODEL_NAME = \"qwen3-30b-a3b-instruct-2507\"\n",
    "\n",
    "# 2. å¹¶å‘æ•°é‡\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# 3. æ–‡ä»¶è·¯å¾„\n",
    "try:\n",
    "    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# è¾“å…¥æ–‡ä»¶ (ä½¿ç”¨ä½ åˆšåˆšä¸Šä¼ çš„æ–‡ä»¶)\n",
    "INPUT_FILENAME = \"api_retrieval_split_result_v2.xlsx\" \n",
    "INPUT_PATH = os.path.join(CURRENT_DIR, INPUT_FILENAME)\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶ (ç”Ÿæˆä¸€ä¸ªæ–°æ–‡ä»¶)\n",
    "OUTPUT_PATH = os.path.join(CURRENT_DIR, \"final_analysis_comparison_aliyun_v2.xlsx\")\n",
    "\n",
    "# ===========================================\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
    "client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_API_BASE)\n",
    "\n",
    "def clean_model_output(text):\n",
    "    \"\"\"æ¸…æ´—æ¨¡å‹è¾“å‡º\"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    text = text.replace('```', '').strip()\n",
    "    return text\n",
    "\n",
    "def merge_columns(row, prefix, count):\n",
    "    \"\"\"åˆå¹¶å¤šä¸ªå¬å›ç»“æœåˆ—\"\"\"\n",
    "    content_list = []\n",
    "    for i in range(1, count + 1):\n",
    "        col_name = f\"{prefix}{i}\"\n",
    "        if col_name not in row and prefix == \"å¬å›ç»“æœ\":\n",
    "             col_name = f\"å¬å›ç»“æœ{i}\"\n",
    "\n",
    "        if col_name in row and pd.notna(row[col_name]):\n",
    "            val = str(row[col_name]).strip()\n",
    "            # ç®€å•å»é‡å’Œå»ç©º\n",
    "            if val and val not in content_list:\n",
    "                content_list.append(f\"[{i}]: {val}\")\n",
    "    \n",
    "    # é»˜è®¤æˆªå– 2500 å­—\n",
    "    return \"\\n\".join(content_list)[:2500]\n",
    "\n",
    "def call_llm_smart(prompt, retries=3):\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½è°ƒç”¨å‡½æ•°ï¼šåŒ…å«é‡è¯• + åˆè§„æ‹¦æˆªè‡ªåŠ¨é™çº§æœºåˆ¶\n",
    "    \"\"\"\n",
    "    current_prompt = prompt\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„æ•°æ®åˆ†æå¸ˆã€‚è¯·ç›´æ¥è¾“å‡ºåˆ†æç»“æœï¼Œä¸¥ç¦åŒ…å«æ€è€ƒè¿‡ç¨‹(<think>)ï¼Œä¸¥ç¦è¾“å‡ºå®¢å¥—è¯ã€‚å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚\"},\n",
    "                    {\"role\": \"user\", \"content\": current_prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=256\n",
    "            )\n",
    "            return clean_model_output(chat_response.choices[0].message.content)\n",
    "            \n",
    "        except BadRequestError as e:\n",
    "            # æ•è· 400 é”™è¯¯ (åŒ…å« data_inspection_failed)\n",
    "            err_body = e.body or {}\n",
    "            err_code = err_body.get('code', '')\n",
    "            \n",
    "            # å¦‚æœæ˜¯åˆè§„æ£€æµ‹å¤±è´¥ (data_inspection_failed)\n",
    "            if 'data_inspection' in str(e) or 'inappropriate content' in str(e):\n",
    "                print(f\"âš ï¸ è§¦å‘åˆè§„æ‹¦æˆªï¼Œå°è¯•ç¼©å‡æ–‡æœ¬é‡è¯• ({i+1}/{retries})...\")\n",
    "                # ã€å…³é”®ç­–ç•¥ã€‘: å°† Prompt é•¿åº¦ç åŠï¼Œè¯•å›¾ä¸¢å¼ƒåŒ…å«æ•æ„Ÿè¯çš„éƒ¨åˆ†\n",
    "                if len(current_prompt) > 500:\n",
    "                    current_prompt = current_prompt[:len(current_prompt)//2] + \"\\n...(ååŠéƒ¨åˆ†å› é£æ§æˆªæ–­)\"\n",
    "                    continue # ä½¿ç”¨ç¼©çŸ­åçš„ prompt ç«‹å³é‡è¯•\n",
    "                else:\n",
    "                    return \"å†…å®¹è§¦å‘å¹³å°é£æ§ï¼Œæ— æ³•è¯„ä¼°\"\n",
    "            else:\n",
    "                return f\"è¯·æ±‚å‚æ•°é”™è¯¯: {str(e)[:50]}\"\n",
    "                \n",
    "        except (APIConnectionError, RateLimitError) as e:\n",
    "            # ç½‘ç»œæˆ–é™æµé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"æœªçŸ¥é”™è¯¯: {str(e)[:50]}\"\n",
    "            \n",
    "    return \"è°ƒç”¨è¶…æ—¶æˆ–å¤šæ¬¡å¤±è´¥\"\n",
    "\n",
    "def analyze_single_row(row):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•è¡Œæ•°æ®\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å¦‚æœä¹‹å‰å·²ç»æœ‰æˆåŠŸçš„ç»“æœï¼Œè·³è¿‡ï¼ˆæ–¹ä¾¿æ–­ç‚¹ç»­ä¼ ï¼‰\n",
    "        # è¿™é‡Œå‡è®¾ä½ è¦é‡æ–°è·‘ï¼Œæ‰€ä»¥ä¸åšè·³è¿‡æ£€æŸ¥\n",
    "        \n",
    "        question = str(row.get('é—®é¢˜', row.get('åŸé—®é¢˜', '')))\n",
    "        \n",
    "        baseline_content = merge_columns(row, \"å¬å›ç»“æœ\", 5)\n",
    "        scheme1_content = merge_columns(row, \"æ–¹æ¡ˆ1_å¬å›ç»“æœ\", 3)\n",
    "        scheme2_content = merge_columns(row, \"æ–¹æ¡ˆ2_å¬å›ç»“æœ\", 3)\n",
    "\n",
    "        if not baseline_content: baseline_content = \"ï¼ˆæ— å¬å›å†…å®¹ï¼‰\"\n",
    "        if not scheme1_content: scheme1_content = \"ï¼ˆæ— å¬å›å†…å®¹ï¼‰\"\n",
    "        if not scheme2_content: scheme2_content = \"ï¼ˆæ— å¬å›å†…å®¹ï¼‰\"\n",
    "\n",
    "        # æ„é€  Prompts\n",
    "        prompts = [\n",
    "            # 1. æ–¹æ¡ˆ1 è¯„ä¼°\n",
    "            f\"ã€é—®é¢˜ã€‘ï¼š{question}\\nã€æ£€ç´¢å†…å®¹ã€‘ï¼š{scheme1_content}\\nè¯·ä¸€å¥è¯è¯„ä¼°ï¼šä¸Šè¿°æ£€ç´¢å†…å®¹æ˜¯å¦åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„å…³é”®äº‹å®ï¼Ÿç¼ºå¤±äº†ä»€ä¹ˆï¼Ÿ(é™100å­—)\",\n",
    "            # 2. æ–¹æ¡ˆ2 è¯„ä¼°\n",
    "            f\"ã€é—®é¢˜ã€‘ï¼š{question}\\nã€æ£€ç´¢å†…å®¹ã€‘ï¼š{scheme2_content}\\nè¯·ä¸€å¥è¯è¯„ä¼°ï¼šä¸Šè¿°æ£€ç´¢å†…å®¹æ˜¯å¦åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„å…³é”®äº‹å®ï¼Ÿç¼ºå¤±äº†ä»€ä¹ˆï¼Ÿ(é™100å­—)\",\n",
    "            # 3. æ–¹æ¡ˆ1 vs åŸå¬å›\n",
    "            f\"ã€é—®é¢˜ã€‘ï¼š{question}\\nã€åŸæ£€ç´¢ã€‘ï¼š{baseline_content}\\nã€æ–°æ£€ç´¢(é•¿éš¾å¥æ‰©å†™)ã€‘ï¼š{scheme1_content}\\nè¯·å¯¹æ¯”ï¼šæ–°æ£€ç´¢ç›¸æ¯”åŸæ£€ç´¢ï¼Œå‡†ç¡®æ€§æ˜¯æå‡è¿˜æ˜¯ä¸‹é™ï¼Ÿä¸»è¦ä½“ç°åœ¨å“ªï¼Ÿ(é™100å­—ï¼Œç›´æ¥ç»™ç»“è®º)\",\n",
    "            # 4. æ–¹æ¡ˆ2 vs åŸå¬å›\n",
    "            f\"ã€é—®é¢˜ã€‘ï¼š{question}\\nã€åŸæ£€ç´¢ã€‘ï¼š{baseline_content}\\nã€æ–°æ£€ç´¢(æ½œåœ¨è¿½é—®)ã€‘ï¼š{scheme2_content}\\nè¯·å¯¹æ¯”ï¼šæ–°æ£€ç´¢ç›¸æ¯”åŸæ£€ç´¢ï¼Œä¿¡æ¯ä¸°å¯Œåº¦æ˜¯æå‡è¿˜æ˜¯ä¸‹é™ï¼Ÿæ˜¯å¦å¼•å…¥äº†å™ªéŸ³ï¼Ÿ(é™100å­—ï¼Œç›´æ¥ç»™ç»“è®º)\"\n",
    "        ]\n",
    "\n",
    "        # ä¸²è¡Œæ‰§è¡Œ4æ¬¡è°ƒç”¨ (å•è¡Œå†…ä¸²è¡Œï¼Œè¡Œä¸è¡Œä¹‹é—´å¹¶è¡Œ)\n",
    "        results = []\n",
    "        for p in prompts:\n",
    "            res = call_llm_smart(p)\n",
    "            results.append(res)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        err_msg = f\"å¤„ç†å¼‚å¸¸: {str(e)}\"\n",
    "        return [err_msg, err_msg, err_msg, err_msg]\n",
    "\n",
    "def main():\n",
    "    print(f\"æ­£åœ¨è¯»å–æ–‡ä»¶: {INPUT_PATH}\")\n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        print(\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_excel(INPUT_PATH)\n",
    "    total_rows = len(df)\n",
    "    print(f\"å…± {total_rows} æ¡æ•°æ®ï¼Œä½¿ç”¨ {MAX_WORKERS} çº¿ç¨‹å¹¶å‘å¤„ç† (å«æ™ºèƒ½å®¹é”™)...\")\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        rows = df.to_dict('records')\n",
    "        results_iterator = list(tqdm(executor.map(analyze_single_row, rows), total=total_rows, desc=\"LLMæ™ºèƒ½åˆ†æä¸­\"))\n",
    "        results_list = results_iterator\n",
    "\n",
    "    # è§£åŒ…ç»“æœ\n",
    "    col1, col2, col3, col4 = [], [], [], []\n",
    "    for res in results_list:\n",
    "        col1.append(res[0])\n",
    "        col2.append(res[1])\n",
    "        col3.append(res[2])\n",
    "        col4.append(res[3])\n",
    "\n",
    "    df[\"æ–¹æ¡ˆ1_æ•ˆæœè¯„ä¼°\"] = col1\n",
    "    df[\"æ–¹æ¡ˆ2_æ•ˆæœè¯„ä¼°\"] = col2\n",
    "    df[\"æ–¹æ¡ˆ1_vs_åŸå¬å›_å¯¹æ¯”\"] = col3\n",
    "    df[\"æ–¹æ¡ˆ2_vs_åŸå¬å›_å¯¹æ¯”\"] = col4\n",
    "\n",
    "    print(\"åˆ†æå®Œæˆï¼Œæ­£åœ¨ä¿å­˜...\")\n",
    "    try:\n",
    "        df.to_excel(OUTPUT_PATH, index=False)\n",
    "        print(f\"ğŸ‰ å¤„ç†æˆåŠŸï¼ç»“æœä¿å­˜åœ¨: {OUTPUT_PATH}\")\n",
    "    except PermissionError:\n",
    "        print(\"âŒ ä¿å­˜å¤±è´¥ï¼šè¯·å…³é—­ Excel æ–‡ä»¶åé‡è¯•ã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
