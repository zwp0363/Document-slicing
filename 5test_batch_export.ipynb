{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ee2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 找到 23 个 .doc 文件待处理\n",
      "开始批量处理...\n",
      "\n",
      "[1/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\20141022复制新经济？.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 20141022复制新经济？.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 5)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 24 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 252)\n",
      "内容: 〖特别报告〗        复制新经济？ 摘要：新经济具有四大基本特征：看不上的都是“对的”，高高在上的都是不成的，小国寡民都是玩不转的，与人（体验）无关都是不...\n",
      "\n",
      ">>> Chunk 1 (长度: 403)\n",
      "内容: 对于这个新世界，人人皆知要转变思维方式，但新的思维方式是什么却并不清楚，往往还想用老经济的老思想，来理解新经济的新思想。比如当下最为炙手可热的莫过于互联网思维了...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\20141022复制新经济？.xlsx\n",
      "\n",
      "[2/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\20170322手游业爆发前夕.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 20170322手游业爆发前夕.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 5)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 27 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 95 < 100。\n",
      "   -> (Chunk 0) 长度 95 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 95)\n",
      "内容: 〖特别报告〗         手游业爆发前夕 摘要：就人性而言，越是现实得不到的体验，越容易沉醉于另一个世界的“虚妄”，而且往往现实世界有多无奈，虚拟世界就有多...\n",
      "\n",
      ">>> Chunk 1 (长度: 564)\n",
      "内容: “爽快厮杀，淋漓战斗”，手游业强势来袭。2016年夏天，《PokemonGo》风靡全球：美国患22年抑郁症和社交恐惧症的“死宅”为了捉精灵一天走了20公里，日本...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\20170322手游业爆发前夕.xlsx\n",
      "\n",
      "[3/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\20181221服务业品牌信任危机——阶级化还是职业化？.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 20181221服务业品牌信任危机——阶级化还是职业化？.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 26 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 104)\n",
      "内容: 〖特别报告〗服务业品牌信任危机——阶级化还是职业化？ 摘要：服务业品牌信任危机在当下集中爆发是当前特殊时代背景叠加作用的结果。若任由完全市场化逻辑，自然产生“阶...\n",
      "\n",
      ">>> Chunk 1 (长度: 495)\n",
      "内容: 忽如一夜春风来，服务业品牌纷纷陷入危机。11月15日晚上，微博名人“花总丢了金箍棒”发布视频《杯子的秘密》曝光14家五星级酒店卫生保洁操作违规，床单、毛巾和马桶...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\20181221服务业品牌信任危机——阶级化还是职业化？.xlsx\n",
      "\n",
      "[4/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\yq2020-0623智能化就是无人化，服务业能否成为主战场？——劳动力从市场到社会.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2020-0623智能化就是无人化，服务业能否成为主战场？——劳动力从市场到社会.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 22 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 106)\n",
      "内容: 〖特别报告〗智能化就是无人化，服务业能否成为主战场？ ——劳动力从市场到社会 摘要：无人化对就业的冲击已波及服务业，就业结构面临着调整。但因市场经济的局限性，劳...\n",
      "\n",
      ">>> Chunk 1 (长度: 504)\n",
      "内容: 智能化的发展为无人化提供了更多的可能。智能化在各行各业已经有了广泛的应用。在零售业，可以通过智能化软硬一体算力来实现精准营销；金融业通过风控智能化的方式来帮助客...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\yq2020-0623智能化就是无人化，服务业能否成为主战场？——劳动力从市场到社会.xlsx\n",
      "\n",
      "[5/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\yq2021-0426世界性焦虑症——贩卖焦虑，治愈经济.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0426世界性焦虑症——贩卖焦虑，治愈经济.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 20 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "   -> 检测到长段落 (长度 1225)，尝试按中间句号 '。' 切分...\n",
      "   -> S 成功切分为两块: (长度 572) 和 (长度 653)\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 94 < 100。\n",
      "   -> (Chunk 0) 长度 94 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 94)\n",
      "内容: 〖特别报告〗世界性焦虑症——贩卖焦虑，治愈经济 摘要：世界性焦虑与商业消费主义等刻意贩卖焦虑脱不了干系，但更是国家间激烈竞争、国家内部急剧转型等的必然结果，如何...\n",
      "\n",
      ">>> Chunk 1 (长度: 610)\n",
      "内容: 如果网络投票评选最热门的词，“焦虑”一定名列前茅。普罗大众除了背负传统的教育、医疗、住房、养老“四座大山”，还难以避免地被社交、婚恋、职场、年龄、健康等或多或少...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-1新型服务业\\yq2021-0426世界性焦虑症——贩卖焦虑，治愈经济.xlsx\n",
      "\n",
      "[6/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2019-1106世界大政治从军备到环保(郭晓丹).doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2019-1106世界大政治从军备到环保(郭晓丹).doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 20 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 12 个块，正在进行小块合并...\n",
      "清理完成，最终得到 12 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 421)\n",
      "内容: 〖特别报告〗    世界大政治：从军备到环保 摘要：环保问题成为世界政治的最大公约数。“空气正在以它看不见、摸不着、蔑视一切边界的特质，挑战着过去一切既定的政治...\n",
      "\n",
      ">>> Chunk 1 (长度: 372)\n",
      "内容: 毕竟，“秀肌肉”历来是美国力量展示的惯常手段，军事立国更是其多年来称霸全球的头号准则。正如2009年美国总统奥巴马接受诺贝尔和平奖时所说的那样，“我们必须从现在...\n",
      "\n",
      "... (及其他 10 个块)\n",
      "\n",
      "正在将 12 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 12 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2019-1106世界大政治从军备到环保(郭晓丹).xlsx\n",
      "\n",
      "[7/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2019-1118亩产过吨与西北绿洲——人算与天算（杨国振）.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2019-1118亩产过吨与西北绿洲——人算与天算（杨国振）.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 18 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 11 个块，正在进行小块合并...\n",
      "清理完成，最终得到 11 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 397)\n",
      "内容: 〖特别报告〗  亩产过吨与西北绿洲——人算与天算 摘要：在气候恶化的大趋势下，“人算”越甚、“人祸”越大，则危机越重，出路在于生活方式与消费内容的升级。 201...\n",
      "\n",
      ">>> Chunk 1 (长度: 268)\n",
      "内容: 西北留给人们的普遍印象是荒芜。但近日，甘肃敦煌的科研人员在库姆塔格沙漠东缘发现了消失了数百年的哈拉奇湖，面积有5平方公里，水草丰美、堪称奇迹。科学研究表明，西北...\n",
      "\n",
      "... (及其他 9 个块)\n",
      "\n",
      "正在将 11 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 11 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2019-1118亩产过吨与西北绿洲——人算与天算（杨国振）.xlsx\n",
      "\n",
      "[8/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2021-0204碳中和产业变革倒逼底线.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0204碳中和产业变革倒逼底线.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 23 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "   -> 检测到长段落 (长度 1004)，尝试按中间句号 '。' 切分...\n",
      "   -> S 成功切分为两块: (长度 487) 和 (长度 517)\n",
      "初步切割得到 8 个块，正在进行小块合并...\n",
      "清理完成，最终得到 8 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 101)\n",
      "内容: 〖特别报告〗碳中和产业变革倒逼底线 摘要：随着全球各国相继提出净零排放的碳中和目标，势必加速全球产业结构变革。碳中和将倒逼所有行业的底线，是否符合碳中和将成为衡...\n",
      "\n",
      ">>> Chunk 1 (长度: 527)\n",
      "内容: 2020年，一系列异常自然灾害被淹没在新冠疫情的报道里：年初，澳大利亚山火，横扫近十万平方公里，夺走了约五亿动物生命，这场由全球变暖引起的灾难，向大气排放了约数...\n",
      "\n",
      "... (及其他 6 个块)\n",
      "\n",
      "正在将 8 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 8 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2021-0204碳中和产业变革倒逼底线.xlsx\n",
      "\n",
      "[9/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2021-0512复式时代的回程票——垃圾是个标准.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0512复式时代的回程票——垃圾是个标准.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 27 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "   -> 检测到长段落 (长度 1054)，尝试按中间句号 '。' 切分...\n",
      "   -> S 成功切分为两块: (长度 443) 和 (长度 611)\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 128)\n",
      "内容: 〖特别报告〗复式时代的“回程票”——垃圾是个标准 摘要：当前城市中，不只是垃圾问题，过去诸多城市发展也犹如一张“单程票”，城市循环性变差，发展与环境出现脱钩。面...\n",
      "\n",
      ">>> Chunk 1 (长度: 548)\n",
      "内容: 曾经被称为“史上最严垃圾分类标准”的《上海市生活垃圾管理条例》（以下简称《条例》）实施即将两周年。在一周年时，官方曾公布过一组数据展示上海垃圾分类投放取得的成效...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2021-0512复式时代的回程票——垃圾是个标准.xlsx\n",
      "\n",
      "[10/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2021-0519互联网碳中和、碳达峰的路线图.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0519互联网碳中和、碳达峰的路线图.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 20 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 95 < 100。\n",
      "   -> (Chunk 0) 长度 95 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 95)\n",
      "内容: 〖特别报告〗互联网碳中和、碳达峰的路线图 摘要：剧增的用电量与碳排放成为互联网行业高速发展的“隐形杠杆”。互联网要实现碳达峰、碳中和，是技术、产业、城市等各个步...\n",
      "\n",
      ">>> Chunk 1 (长度: 693)\n",
      "内容: 互联网行业发展至今一直贴着“高端”、“绿色”、“环保”的标签，说到传统行业节能转型必得拉上互联网，提及云存储、云计算等广告宣传也常与“蓝天白云”相关联。可剧增的...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-2节能环保\\yq2021-0519互联网碳中和、碳达峰的路线图.xlsx\n",
      "\n",
      "[11/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-0519互联网碳中和、碳达峰的路线图.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0519互联网碳中和、碳达峰的路线图.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 20 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 95 < 100。\n",
      "   -> (Chunk 0) 长度 95 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 95)\n",
      "内容: 〖特别报告〗互联网碳中和、碳达峰的路线图 摘要：剧增的用电量与碳排放成为互联网行业高速发展的“隐形杠杆”。互联网要实现碳达峰、碳中和，是技术、产业、城市等各个步...\n",
      "\n",
      ">>> Chunk 1 (长度: 693)\n",
      "内容: 互联网行业发展至今一直贴着“高端”、“绿色”、“环保”的标签，说到传统行业节能转型必得拉上互联网，提及云存储、云计算等广告宣传也常与“蓝天白云”相关联。可剧增的...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-0519互联网碳中和、碳达峰的路线图.xlsx\n",
      "\n",
      "[12/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-0609躲得过市场洗牌，躲不过碳中和？——六大产业的命门.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0609躲得过市场洗牌，躲不过碳中和？——六大产业的命门.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 25 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 9 个块，正在进行小块合并...\n",
      "清理完成，最终得到 9 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 123)\n",
      "内容: 〖特别报告〗 躲得过市场洗牌，躲不过碳中和？ ——六大产业的命门 摘要：许多企业还未从百年一遇之大变局叠加新冠疫情冲击带来的市场洗牌中缓过神，碳中和引领的时代变...\n",
      "\n",
      ">>> Chunk 1 (长度: 422)\n",
      "内容: 2020年，由于百年一遇之大变局叠加新冠疫情的影响，各行各业都面临着残酷的市场洗牌，如许多行业本就面临着严峻的产能过剩问题，行业出清加速，再叠加国际贸易摩擦、新...\n",
      "\n",
      "... (及其他 7 个块)\n",
      "\n",
      "正在将 9 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 9 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-0609躲得过市场洗牌，躲不过碳中和？——六大产业的命门.xlsx\n",
      "\n",
      "[13/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-0907避免“双碳”运动化的两大变量.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0907避免“双碳”运动化的两大变量.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 21 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 94 < 100。\n",
      "   -> (Chunk 0) 长度 94 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 94)\n",
      "内容: 〖特别报告〗避免“双碳”运动化的两大变量 摘要：一场低碳战役已然拉开帷幕，不仅需要能源领域的技术创新、效率提高，更需要政策、市场与治理相互协同，发挥好“有形的手...\n",
      "\n",
      ">>> Chunk 1 (长度: 475)\n",
      "内容: 8月17日，国家发改委召开例行新闻发布会，会上明确表示，对与碳达峰、碳中和工作的初衷和要求背道而驰的现象，必须坚决予以纠正，既要纠正运动式“减碳”，先立后破，也...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-0907避免“双碳”运动化的两大变量.xlsx\n",
      "\n",
      "[14/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-1027碳博弈：从税制、市场到规则.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-1027碳博弈：从税制、市场到规则.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 16 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 13 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 85 < 100。\n",
      "   -> (Chunk 0) 长度 85 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 13 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 85)\n",
      "内容: 〖特别报告〗碳博弈：从税制、市场到规则 摘要：就像石油曾被作为大国博弈的工具一样，碳减排、碳中和已成为大国辗转腾挪的新棋子，无论是碳税还是碳交易市场，归根结底是...\n",
      "\n",
      ">>> Chunk 1 (长度: 516)\n",
      "内容: 2011年末，欧盟不顾世界上绝大多数国家的反对，单边宣布将于2012年1月1日征收进入欧盟机场降落的航空器碳排放税。中国对此也强烈抵制，欧盟和中国关于航空碳税的...\n",
      "\n",
      "... (及其他 11 个块)\n",
      "\n",
      "正在将 13 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 13 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-1027碳博弈：从税制、市场到规则.xlsx\n",
      "\n",
      "[15/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-1119“双碳”突破战略方向在哪儿？——小微藻、大能量.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-1119“双碳”突破战略方向在哪儿？——小微藻、大能量.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 24 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 93 < 100。\n",
      "   -> (Chunk 0) 长度 93 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 93)\n",
      "内容: 〖特别报告〗 “双碳”突破战略方向在哪儿？ ——小微藻、大能量 摘要：就像1913年，福特第一条汽车流水线上线，揭开了现代化大生产的序幕；微藻革命也将拉开“双碳...\n",
      "\n",
      ">>> Chunk 1 (长度: 436)\n",
      "内容: 第26届《联合国气候变化框架公约》缔约方大会（COP26）于10月31日开幕，本次大会的主要目标中，“确保本世纪中叶全球净零碳排放和1.5℃温升目标可及”赫然排...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-3双碳经济\\yq2021-1119“双碳”突破战略方向在哪儿？——小微藻、大能量.xlsx\n",
      "\n",
      "[16/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\200306192004年开始新劫难——知识产权围堵浪潮.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 200306192004年开始新劫难——知识产权围堵浪潮.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 6)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 32 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 239)\n",
      "内容: 〖特别报告〗    2004年开始新劫难 ——知识产权围堵浪潮 2002年中国外贸形势出人意料，原因在于美欧没想到入世后中国廉价商品来势如此迅猛。待回过味来欲在...\n",
      "\n",
      ">>> Chunk 1 (长度: 394)\n",
      "内容: 世界各国对中国商品的围堵之势已渐露端倪。首先是反倾销、反补贴、保障措施三大常规手段使用频繁、力度加大。2002年共有18个国家和地区对中国反倾销和保障措施立案6...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\200306192004年开始新劫难——知识产权围堵浪潮.xlsx\n",
      "\n",
      "[17/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\20061205为什么所有发达国家围剿中国？.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 20061205为什么所有发达国家围剿中国？.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 5)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 31 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 379)\n",
      "内容: 〖特别报告〗为什么所有发达国家围剿中国？ ——“八国联军”将遭遇何种中国功夫？ 摘要：技术霸权穿上了世贸组织“文明”、“正义”的外衣 中国的企业滞后、商务部示弱...\n",
      "\n",
      ">>> Chunk 1 (长度: 537)\n",
      "内容: 从近一个月来的外贸官司不难看出，市场开放度、知识产权保护力度和竞争公平度是发达国家攻击中国的火力集中点，尤以知识产权保护为最。实际上，改革开放之初中国在这三方面...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\20061205为什么所有发达国家围剿中国？.xlsx\n",
      "\n",
      "[18/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\20120704山寨与专利，两利相权谁为重？.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 20120704山寨与专利，两利相权谁为重？.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 5)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 25 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 8 个块，正在进行小块合并...\n",
      "清理完成，最终得到 8 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 463)\n",
      "内容: 〖特别报告〗     山寨与专利，两利相权谁为重？                ——知识产权能否与WTO比肩 摘要：中国经济已到了新的十字路口，政策层面还在继...\n",
      "\n",
      ">>> Chunk 1 (长度: 363)\n",
      "内容: “山寨风行”的背后虽是国人善巧方便的本性，但也离不开WTO的大背景。入世以来，中国凭借低成本成为全球制造业转移的主阵地，可以说全球化孕育了中国山寨模式，而低成本...\n",
      "\n",
      "... (及其他 6 个块)\n",
      "\n",
      "正在将 8 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 8 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\20120704山寨与专利，两利相权谁为重？.xlsx\n",
      "\n",
      "[19/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\20190604知识产权的两面性——中国两难.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: 20190604知识产权的两面性——中国两难.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 21 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 87 < 100。\n",
      "   -> (Chunk 0) 长度 87 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 87)\n",
      "内容: 〖特别报告〗知识产权的两面性——中国两难 摘要：知识产权作为保护原创、鼓励创新的手段正在渐渐露出它“不光明”的背面；而中国作为该领域的后起之秀，所面临的两难境地...\n",
      "\n",
      ">>> Chunk 1 (长度: 422)\n",
      "内容: 前不久人类首张黑洞照片被公布，这本该载入科学史册的大事件却被视觉中国的“独家声明”抢了风头。视觉中国的操作让企业和媒体“头疼不已”：“维权获客”只是平常，“钓鱼...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\20190604知识产权的两面性——中国两难.xlsx\n",
      "\n",
      "[20/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\yq2021-0319文艺作品从流量为王伪市场到专利为准真市场.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0319文艺作品从流量为王伪市场到专利为准真市场.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 28 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 110)\n",
      "内容: 〖特别报告〗文艺作品从流量为王伪市场到专利为准真市场 摘要：信息文明背景下，流量对于文艺作品实现价值变现作用陡增，却也刺激了流量的造假和对内容的忽视，只有以法律...\n",
      "\n",
      ">>> Chunk 1 (长度: 444)\n",
      "内容: 近年来，《战狼II》、《我不是药神》、《流浪地球》、《山海情》等影视佳作纷纷问世，或着眼于家国天下，或聚焦于民生现实，无意追求流量变现却成为一时之间的爆款。20...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-4知识产权\\yq2021-0319文艺作品从流量为王伪市场到专利为准真市场.xlsx\n",
      "\n",
      "[21/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-5元宇宙\\yq2021-0706从互联网到物联网，再到元宇宙——迭代还是忽悠？.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-0706从互联网到物联网，再到元宇宙——迭代还是忽悠？.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 22 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 6 个块，正在进行小块合并...\n",
      "清理完成，最终得到 6 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 106)\n",
      "内容: 〖特别报告〗从互联网到物联网，再到元宇宙 ——迭代还是忽悠？ 摘要：元宇宙尚属信息文明的“树枝”，未形成普遍认可度，可以说兼具“迭代”和“忽悠”属性，但科技在发...\n",
      "\n",
      ">>> Chunk 1 (长度: 535)\n",
      "内容: “元宇宙”一词源于1992年尼尔·斯蒂芬森的科幻小说《雪崩》，该小说描绘了一个脱胎于现实世界，又与现实世界平行、相互影响，并且始终在线的虚拟世界，人类以“数字化...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在将 6 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 6 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-5元宇宙\\yq2021-0706从互联网到物联网，再到元宇宙——迭代还是忽悠？.xlsx\n",
      "\n",
      "[22/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-5元宇宙\\yq2021-1207为元宇宙准备了半个多世纪——技术集成还是变局集成？（上）.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-1207为元宇宙准备了半个多世纪——技术集成还是变局集成？（上）.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 22 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "   -> 检测到长段落 (长度 1237)，尝试按中间句号 '。' 切分...\n",
      "   -> S 成功切分为两块: (长度 624) 和 (长度 613)\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "   -> 提示：第一个块 (Chunk 0) 长度 84 < 100。\n",
      "   -> (Chunk 0) 长度 84 太小，但无法与下一块合并 (会超长)。\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 84)\n",
      "内容: 〖特别报告〗为元宇宙准备了半个多世纪 ——技术集成还是变局集成？（上） 摘要：元宇宙的爆红，既是人们关于未来的想象投射，也有疫情的推波助澜，更离不开庞大技术体系...\n",
      "\n",
      ">>> Chunk 1 (长度: 495)\n",
      "内容: “世界的尽头是铁岭，互联网的尽头是元宇宙。”上至科技巨头，下至创业公司，企业纷纷“抢滩”这一最火概念。今年3月，游戏开发平台Roblox在纳斯达克上市，上市第一...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-5元宇宙\\yq2021-1207为元宇宙准备了半个多世纪——技术集成还是变局集成？（上）.xlsx\n",
      "\n",
      "[23/23] 准备处理...\n",
      "\n",
      "------------------------------------------------------------\n",
      "处理文件: F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-5元宇宙\\yq2021-1215为元宇宙准备了半个多世纪——技术集成还是变局集成？（下）.doc\n",
      "------------------------------------------------------------\n",
      "正在加载文档: yq2021-1215为元宇宙准备了半个多世纪——技术集成还是变局集成？（下）.doc...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 找到正文起始标记 '〖特别报告〗' (位于段落 0)。\n",
      "   -> 已成功清理正文标题行。\n",
      "   -> 已处理 24 个正文段落。\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "   -> 检测到长段落 (长度 1051)，尝试按中间句号 '。' 切分...\n",
      "   -> S 成功切分为两块: (长度 472) 和 (长度 579)\n",
      "初步切割得到 7 个块，正在进行小块合并...\n",
      "清理完成，最终得到 7 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 106)\n",
      "内容: 〖特别报告〗为元宇宙准备了半个多世纪 ——技术集成还是变局集成？（下） 摘要：未来，在技术突破之外，还要从价值确定、经济规则，到治理和分配政策、道德伦理标准等方...\n",
      "\n",
      ">>> Chunk 1 (长度: 711)\n",
      "内容: 元宇宙预示的未来充满机遇和想象，热潮汹涌下，创业者跑步入场，投资人宿夜难寐，大厂争相注册商标，普通人开始造梗、狂欢，谁都不愿错过这个难得的新风口。有人还没搞懂元...\n",
      "\n",
      "... (及其他 5 个块)\n",
      "\n",
      "正在将 7 个块转换为 DataFrame...\n",
      "\n",
      "✅ 任务完成！已成功导出 7 条数据到：\n",
      "F:\\福卡\\福卡知识库测试文件\\20新兴领域\\20-5元宇宙\\yq2021-1215为元宇宙准备了半个多世纪——技术集成还是变局集成？（下）.xlsx\n",
      "\n",
      "------------------------------------------------------------\n",
      "批量处理完成！\n",
      "总文件数: 23\n",
      "成功处理: 23\n",
      "处理失败: 0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. 配置 ---\n",
    "# 批量处理模式：指定文件夹路径\n",
    "SOURCE_DIR = \"F:\\\\福卡\\\\福卡知识库测试文件\\\\20新兴领域\"  # 替换为您要处理的文件夹路径\n",
    "# 单文件模式（保留以便单独测试）\n",
    "SOURCE_DOC_FILE = \"F:\\\\福卡\\\\福卡知识库测试文件\\\\2企业战略\\\\2-3商业模式\\\\20120702商业模式投机？.doc\"\n",
    "# 切割配置\n",
    "MAX_CHUNK_LEN = 500  # 合并的目标上限\n",
    "MIN_CHUNK_LEN = 100  # 合并的最小阈值\n",
    "HARD_MAX_LEN = 1000 # 单个段落的硬性拆分上限\n",
    "\n",
    "\n",
    "# --- 2. 文档加载 ---\n",
    "def find_libreoffice():\n",
    "    possible_paths = [\n",
    "        r\"C:\\\\Program Files\\\\LibreOffice\\\\program\\soffice.exe\",\n",
    "        r\"C:\\\\Program Files (x86)\\\\LibreOffice\\\\program\\soffice.exe\",\n",
    "        r\"D:\\\\LibreOffice\\\\program\\soffice.exe\",\n",
    "        \"soffice\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path != \"soffice\" and not os.path.exists(path): continue\n",
    "            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0: return path\n",
    "        except: continue\n",
    "    return None\n",
    "\n",
    "def load_doc_as_text(doc_path):\n",
    "    print(f\"正在加载文档: {os.path.basename(doc_path)}...\")\n",
    "    libreoffice_path = find_libreoffice()\n",
    "    if not libreoffice_path:\n",
    "        print(\"错误：未找到 LibreOffice (soffice.exe)，请检查路径。\")\n",
    "        return None\n",
    "    \n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    output_dir = os.path.dirname(doc_path)\n",
    "    html_filename = os.path.basename(doc_path).rsplit('.', 1)[0] + '.html'\n",
    "    html_path = os.path.join(output_dir, html_filename)\n",
    "    \n",
    "    if os.path.exists(html_path): \n",
    "        try: os.remove(html_path)\n",
    "        except: pass\n",
    "        \n",
    "    cmd = [libreoffice_path, '--headless', '--convert-to', 'html', '--outdir', output_dir, doc_path]\n",
    "    subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if not os.path.exists(html_path):\n",
    "        print(f\"错误：LibreOffice 转换失败，未在 {output_dir} 找到 {html_filename}\")\n",
    "        return None\n",
    "    \n",
    "    content = \"\"\n",
    "    for enc in ['utf-8', 'gb18030', 'gbk']:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding=enc) as f:\n",
    "                content = f.read()\n",
    "                break\n",
    "        except: continue\n",
    "            \n",
    "    if not content:\n",
    "        print(\"错误：读取转换后的 HTML 文件失败。\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # 1. 移除页眉和页脚的 div\n",
    "    print(\"   -> 正在移除页眉 (header) 和页脚 (footer)...\")\n",
    "    header = soup.find('div', title='header')\n",
    "    if header:\n",
    "        header.decompose() \n",
    "\n",
    "    footer = soup.find('div', title='footer')\n",
    "    if footer:\n",
    "        footer.decompose()\n",
    "    \n",
    "    # 2. 获取所有非空段落\n",
    "    # 获取所有可能的文本标签，不仅仅是 p\n",
    "    # 包含段落(p), 各种标题(h1-h6), 分块(div), 列表项(li)\n",
    "    target_tags = ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote']\n",
    "    found_tags = soup.find_all(target_tags)\n",
    "    \n",
    "    cleaned_paragraphs = []\n",
    "    for tag in found_tags:\n",
    "        # 1. 获取文本\n",
    "        p_text = tag.get_text()\n",
    "        \n",
    "        # 2. 移除标签内部的换行符\n",
    "        # LibreOffice 会在 HTML 标签内部插入 \\n 进行视觉换行，\n",
    "        # 这对于中文文档会导致句子被错误切断。必须将其替换为空字符串。\n",
    "        p_text = p_text.replace('\\n', '').replace('\\r', '')\n",
    "        \n",
    "        # 3. 去除首尾空白\n",
    "        p_text = p_text.strip()\n",
    "        \n",
    "        if p_text:\n",
    "            cleaned_paragraphs.append(p_text)\n",
    "\n",
    "    # 3. 找到正文开始的标记 \"〖特别报告〗\"，删除之前的内容\n",
    "    start_index = -1\n",
    "    for i, p_text in enumerate(cleaned_paragraphs):\n",
    "        if \"〖特别报告〗\" in p_text:\n",
    "            start_index = i\n",
    "            break\n",
    "    \n",
    "    final_paragraphs = []\n",
    "    if start_index != -1:\n",
    "        # 找到了标记\n",
    "        print(f\"   -> 找到正文起始标记 '〖特别报告〗' (位于段落 {start_index})。\")\n",
    "        \n",
    "        # 3a. 获取 *第一个* 正文段落\n",
    "        first_para_text = cleaned_paragraphs[start_index]\n",
    "        \n",
    "        # 3b. 【新】使用 .split() 来切分，这比 .find() 更健壮\n",
    "        # 这将创建列表: [\"福卡分析...福卡理念：...\", \" 世界改造中国...\"]\n",
    "        parts = first_para_text.split(\"〖特别报告〗\", 1)\n",
    "        \n",
    "        if len(parts) == 2:\n",
    "            # 成功切分. parts[0] 是垃圾页眉, parts[1] 是正文.\n",
    "            # 我们重新组合，保留标记和正文。\n",
    "            cleaned_first_para = \"〖特别报告〗\" + parts[1]\n",
    "            final_paragraphs.append(cleaned_first_para.strip()) # 添加清理后的第一段\n",
    "            print(f\"   -> 已成功清理正文标题行。\")\n",
    "        else:\n",
    "            # 切分失败（极不可能，但作为保险）\n",
    "            print(\"   -> 警告: 'split' 失败，按原样保留段落。\")\n",
    "            final_paragraphs.append(first_para_text)\n",
    "            \n",
    "        # 3c. 添加所有剩余的段落\n",
    "        final_paragraphs.extend(cleaned_paragraphs[start_index + 1:])\n",
    "        print(f\"   -> 已处理 {len(final_paragraphs)} 个正文段落。\")\n",
    "\n",
    "    else:\n",
    "        # 没找到标记\n",
    "        print(\"   -> 警告：未找到 '〖特别报告〗' 标记，将处理所有段落。\")\n",
    "        final_paragraphs = cleaned_paragraphs\n",
    "    \n",
    "    # 4. 用单一的、可靠的 \\n 将所有\"干净\"的段落连接起来\n",
    "    text = \"\\n\".join(final_paragraphs)\n",
    "    \n",
    "    try: \n",
    "        os.remove(html_path) \n",
    "        print(\"   -> 中间 HTML 文件已清理。\")\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# --- 3. 语义切割逻辑 ---\n",
    "def split_long_paragraph(text, hard_max):\n",
    "    print(f\"   -> 检测到长段落 (长度 {len(text)})，尝试按中间句号 '。' 切分...\")\n",
    "    mid_point = len(text) // 2\n",
    "    \n",
    "    pos_before = text.rfind('。', 0, mid_point)\n",
    "    pos_after = text.find('。', mid_point)\n",
    "    \n",
    "    split_pos = -1\n",
    "    \n",
    "    if pos_before != -1 and pos_after != -1:\n",
    "        if (mid_point - pos_before) < (pos_after - mid_point):\n",
    "            split_pos = pos_before\n",
    "        else:\n",
    "            split_pos = pos_after\n",
    "    elif pos_before != -1:\n",
    "        split_pos = pos_before\n",
    "    elif pos_after != -1:\n",
    "        split_pos = pos_after\n",
    "    else:\n",
    "        print(f\"   -> 警告: 段落长度 {len(text)} > {hard_max}，但未找到 '。' 无法切分。\")\n",
    "        return [text] \n",
    "    \n",
    "    part1 = text[:split_pos + 1].strip()\n",
    "    part2 = text[split_pos + 1:].strip()\n",
    "    \n",
    "    if not part1 or not part2:\n",
    "         print(f\"   -> 警告: 按句号切分失败 (产生空块)，返回原长段落。\")\n",
    "         return [text]\n",
    "         \n",
    "    print(f\"   -> S 成功切分为两块: (长度 {len(part1)}) 和 (长度 {len(part2)})\")\n",
    "    return [part1, part2]\n",
    "\n",
    "\n",
    "def split_text_smart(text, max_len=MAX_CHUNK_LEN, min_len=MIN_CHUNK_LEN):\n",
    "    print(f\"正在按段落切割 (合并目标: {max_len}, 最小: {min_len}, 硬上限: {HARD_MAX_LEN})...\")\n",
    "    \n",
    "    # 1. 移除\"相关链接\"\n",
    "    text = re.split(r\"〖相关链接：信息〗|〖相关链接：报告〗|〖参考信息〗|〖参考报告〗\", text, 1)[0]\n",
    "    \n",
    "    # 2. 基础清理\n",
    "    text = re.sub(r'\"', \"\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1f\\x7f-\\x9f]\", \"\", text) # 保留 \\n \\r \\t\n",
    "    text = re.sub(r\"[\\r\\n\\t ]+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text).strip()\n",
    "    \n",
    "    paragraphs = text.split('\\n')\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "    # 3. 长度控制\n",
    "    initial_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # 3a. 检查 > 1000\n",
    "        if len(para) > HARD_MAX_LEN:\n",
    "            sub_paragraphs = split_long_paragraph(para, HARD_MAX_LEN)\n",
    "        else:\n",
    "            sub_paragraphs = [para]\n",
    "\n",
    "        # 3b. 迭代处理(可能被切分后)的段落，并应用 < 500 合并逻辑\n",
    "        for sub_para in sub_paragraphs:\n",
    "            if not sub_para.strip(): continue \n",
    "            \n",
    "            if not current_chunk:\n",
    "                current_chunk = sub_para\n",
    "            elif len(current_chunk) + len(sub_para) + 1 <= max_len: \n",
    "                current_chunk += \"\\n\" + sub_para\n",
    "            else:\n",
    "                initial_chunks.append(current_chunk)\n",
    "                current_chunk = sub_para\n",
    "    \n",
    "    if current_chunk:\n",
    "        initial_chunks.append(current_chunk)\n",
    "        \n",
    "    print(f\"初步切割得到 {len(initial_chunks)} 个块，正在进行小块合并...\")\n",
    "\n",
    "    # 4. 后处理：合并 < 100 的块\n",
    "    final_chunks = []\n",
    "    i = 0\n",
    "    while i < len(initial_chunks):\n",
    "        current_chunk = initial_chunks[i]\n",
    "        \n",
    "        if i == 0 and len(current_chunk) < min_len:\n",
    "             print(f\"   -> 提示：第一个块 (Chunk 0) 长度 {len(current_chunk)} < {min_len}。\")\n",
    "        \n",
    "        if len(current_chunk) < min_len and (i < len(initial_chunks) - 1):\n",
    "            next_chunk = initial_chunks[i+1]\n",
    "            merged_chunk = current_chunk + \"\\n\" + next_chunk\n",
    "            \n",
    "            if len(merged_chunk) <= max_len:\n",
    "                print(f\"   -> (Chunk {i}) 长度 {len(current_chunk)} 太小，已与下一块合并。\")\n",
    "                final_chunks.append(merged_chunk)\n",
    "                i += 2 \n",
    "            else:\n",
    "                print(f\"   -> (Chunk {i}) 长度 {len(current_chunk)} 太小，但无法与下一块合并 (会超长)。\")\n",
    "                final_chunks.append(current_chunk)\n",
    "                i += 1\n",
    "        else:\n",
    "            final_chunks.append(current_chunk)\n",
    "            i += 1\n",
    "\n",
    "    print(f\"清理完成，最终得到 {len(final_chunks)} 个文本块。\")\n",
    "    return final_chunks\n",
    "\n",
    "# --- 5. 新增：批量处理函数 ---\n",
    "def process_single_file(doc_path):\n",
    "    \"\"\"\n",
    "    处理单个文档文件的函数\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"处理文件: {os.path.abspath(doc_path)}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    # 自动将输出Excel放在源文档同一目录\n",
    "    OUTPUT_EXCEL_FILE = os.path.join(os.path.dirname(os.path.abspath(doc_path)), \n",
    "                                    os.path.basename(doc_path).rsplit('.', 1)[0] + '.xlsx')\n",
    "    \n",
    "    # 确保文件存在\n",
    "    if not os.path.exists(doc_path):\n",
    "        print(f\"❌ 错误：源文件未找到！\")\n",
    "        print(f\"请检查文件路径：{os.path.abspath(doc_path)}\")\n",
    "        return False\n",
    "        \n",
    "    full_text = load_doc_as_text(doc_path)\n",
    "    if not full_text: \n",
    "        print(\"❌ 错误：未能加载文档。\")\n",
    "        return False\n",
    "\n",
    "    # 执行切割\n",
    "    chunks = split_text_smart(full_text)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"❌ 错误：切割后未产生任何文本块。\")\n",
    "        return False\n",
    "\n",
    "    # 预览检查（只显示前2个块以减少输出）\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"      最终切分效果预览      \")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < 2: \n",
    "            print(f\"\\n>>> Chunk {i} (长度: {len(chunk)})\")\n",
    "            preview = chunk.replace(\"\\n\", \" \")[:80]\n",
    "            print(f\"内容: {preview}...\")\n",
    "            \n",
    "    if len(chunks) > 2:\n",
    "        print(f\"\\n... (及其他 {len(chunks) - 2} 个块)\")\n",
    "    \n",
    "    \n",
    "    # 转换为 DataFrame\n",
    "    print(f\"\\n正在将 {len(chunks)} 个块转换为 DataFrame...\")\n",
    "    \n",
    "    df = pd.DataFrame(chunks, columns=['text_content'])\n",
    "    df['length'] = df['text_content'].str.len()\n",
    "    df['chunk_id'] = range(1, len(df) + 1)\n",
    "    \n",
    "    # 调整列顺序\n",
    "    df = df[['chunk_id', 'text_content', 'length']]\n",
    "    \n",
    "    # 导出到 Excel\n",
    "    try:\n",
    "        df.to_excel(OUTPUT_EXCEL_FILE, index=False, engine='openpyxl')\n",
    "        print(f\"\\n✅ 任务完成！已成功导出 {len(df)} 条数据到：\")\n",
    "        print(f\"{os.path.abspath(OUTPUT_EXCEL_FILE)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 导出 Excel 失败：{e}\")\n",
    "        print(\"提示：如果文件已打开，请关闭它再重试。\")\n",
    "        return False\n",
    "\n",
    "def find_all_doc_files(directory):\n",
    "    \"\"\"\n",
    "    递归查找目录及其子目录中的所有 .doc 文件\n",
    "    \"\"\"\n",
    "    doc_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.doc') and not file.startswith('~$'):  # 排除临时文件\n",
    "                doc_files.append(os.path.join(root, file))\n",
    "    return doc_files\n",
    "\n",
    "def batch_process():\n",
    "    \"\"\"\n",
    "    批量处理文件夹中的所有doc文件\n",
    "    \"\"\"\n",
    "    if not os.path.exists(SOURCE_DIR):\n",
    "        print(f\"❌ 错误：源文件夹未找到！\")\n",
    "        print(f\"请检查 'SOURCE_DIR' 变量是否指向正确的文件夹：\")\n",
    "        print(f\"{os.path.abspath(SOURCE_DIR)}\")\n",
    "        return\n",
    "    \n",
    "    # 查找所有doc文件\n",
    "    doc_files = find_all_doc_files(SOURCE_DIR)\n",
    "    \n",
    "    if not doc_files:\n",
    "        print(f\"❌ 错误：在指定文件夹中未找到任何 .doc 文件！\")\n",
    "        print(f\"搜索路径：{os.path.abspath(SOURCE_DIR)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ 找到 {len(doc_files)} 个 .doc 文件待处理\")\n",
    "    print(f\"开始批量处理...\")\n",
    "    \n",
    "    # 统计信息\n",
    "    total_files = len(doc_files)\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    failed_files = []\n",
    "    \n",
    "    # 逐个处理文件\n",
    "    for i, doc_file in enumerate(doc_files):\n",
    "        print(f\"\\n[{i+1}/{total_files}] 准备处理...\")\n",
    "        try:\n",
    "            if process_single_file(doc_file):\n",
    "                success_count += 1\n",
    "            else:\n",
    "                failure_count += 1\n",
    "                failed_files.append(doc_file)\n",
    "        except Exception as e:\n",
    "            failure_count += 1\n",
    "            failed_files.append(doc_file)\n",
    "            print(f\"❌ 处理文件时发生异常：{str(e)}\")\n",
    "        \n",
    "        # 可选：每处理完一个文件后暂停一小段时间，避免资源占用过高\n",
    "        # time.sleep(1)\n",
    "    \n",
    "    # 输出处理结果摘要\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"批量处理完成！\")\n",
    "    print(f\"总文件数: {total_files}\")\n",
    "    print(f\"成功处理: {success_count}\")\n",
    "    print(f\"处理失败: {failure_count}\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n❌ 失败的文件列表：\")\n",
    "        for file in failed_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# --- 4. 主程序 ---\n",
    "def main():\n",
    "    # 默认使用批量处理模式\n",
    "    batch_process()\n",
    "    # 如果需要单文件处理，取消下面这行的注释并注释上面的 batch_process() 调用\n",
    "    # process_single_file(SOURCE_DOC_FILE)\n",
    "\n",
    "# --- 运行 ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
