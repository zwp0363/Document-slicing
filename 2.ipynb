{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3121ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\jupyter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84bf726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置 ---\n",
    "SOURCE_DOC_FILE = \"./yq2021-0602文娱产业正离文化越来越远.doc\"\n",
    "\n",
    "# Milvus 配置 (默认使用 Lite 本地模式)\n",
    "USE_MILVUS_LITE = False \n",
    "# 如果您改为 False (使用 Docker)，请设置以下 IP\n",
    "MILVUS_HOST = \"192.168.16.138\" \n",
    "MILVUS_PORT = \"19530\"\n",
    "\n",
    "COLLECTION_NAME = \"report_analysis\"\n",
    "ID_FIELD = \"chunk_id\"\n",
    "VECTOR_FIELD = \"vector\"\n",
    "TEXT_FIELD = \"text_content\"\n",
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# 切割参数控制\n",
    "MAX_CHUNK_LEN = 600  # 超过这个长度强制切分\n",
    "MIN_CHUNK_LEN = 100  # 小于这个长度尝试合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad66dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 文档加载模块 ---\n",
    "def find_libreoffice():\n",
    "    possible_paths = [\n",
    "        r\"C:\\Program Files\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"C:\\Program Files (x86)\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"D:\\LibreOffice\\program\\soffice.exe\",\n",
    "        \"soffice\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path != \"soffice\" and not os.path.exists(path): continue\n",
    "            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0: return path\n",
    "        except: continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f12fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc_as_text(doc_path):\n",
    "    print(\"正在调用 LibreOffice 加载文档...\")\n",
    "    libreoffice_path = find_libreoffice()\n",
    "    if not libreoffice_path: return None\n",
    "    \n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    output_dir = os.path.dirname(doc_path)\n",
    "    html_filename = os.path.basename(doc_path).rsplit('.', 1)[0] + '.html'\n",
    "    html_path = os.path.join(output_dir, html_filename)\n",
    "    \n",
    "    if os.path.exists(html_path): os.remove(html_path)\n",
    "    cmd = [libreoffice_path, '--headless', '--convert-to', 'html', '--outdir', output_dir, doc_path]\n",
    "    subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if not os.path.exists(html_path): return None\n",
    "    \n",
    "    content = \"\"\n",
    "    for enc in ['utf-8', 'gb18030', 'gbk']:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding=enc) as f:\n",
    "                content = f.read()\n",
    "                break\n",
    "        except: continue\n",
    "            \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    try: os.remove(html_path)\n",
    "    except: pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f08710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 智能语义切割 (已修改：丢弃相关链接) ---\n",
    "def split_text_smart(text, max_len=MAX_CHUNK_LEN, min_len=MIN_CHUNK_LEN):\n",
    "    print(f\"正在进行智能语义切割 (Max: {max_len}, Min: {min_len})...\")\n",
    "    \n",
    "    # === [核心修改] 1. 移除 \"相关链接\" 及其后所有内容 ===\n",
    "    # 只要遇到这两个标记中的任何一个，直接截断\n",
    "    text = re.split(r\"〖相关链接：信息〗|〖相关链接：报告〗\", text, 1)[0]\n",
    "    print(\"已移除 '相关链接' 部分。\")\n",
    "    \n",
    "    # 2. 基础清理\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text).strip()\n",
    "    \n",
    "    # 3. 定义语义分隔符 (移除了已被删除的相关链接标记)\n",
    "    # 强制在这些词前面切开\n",
    "    semantic_markers = [\n",
    "        \"摘要：\", \n",
    "        \"一，\", \"二，\", \"三，\", \"四，\", \n",
    "        \"毋庸置疑，\", \"究其原因，\", \"更进一步分析，\"\n",
    "    ]\n",
    "    pattern = f\"(?=({'|'.join(re.escape(m) for m in semantic_markers)}))\"\n",
    "    \n",
    "    # 初步切分\n",
    "    raw_segments = re.split(pattern, text)\n",
    "    raw_segments = [s.strip() for s in raw_segments if s.strip()]\n",
    "    \n",
    "    final_chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # 4. 遍历处理长度\n",
    "    for segment in raw_segments:\n",
    "        if len(segment) > max_len:\n",
    "            if current_chunk:\n",
    "                final_chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            sub_chunks = recursive_split_sentence(segment, max_len)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            is_semantic_start = any(segment.startswith(m) for m in semantic_markers)\n",
    "            \n",
    "            if current_chunk:\n",
    "                # 如果长度允许，且不是新的重要标题，则合并\n",
    "                if len(current_chunk) + len(segment) < max_len and not is_semantic_start:\n",
    "                    current_chunk += \"\\n\" + segment\n",
    "                else:\n",
    "                    # 必须断开。如果上一块太短，且不是强行分隔，尝试粘到这一块头上\n",
    "                    if len(current_chunk) < min_len and not is_semantic_start:\n",
    "                         segment = current_chunk + \"\\n\" + segment\n",
    "                         current_chunk = segment\n",
    "                    else:\n",
    "                        final_chunks.append(current_chunk)\n",
    "                        current_chunk = segment\n",
    "            else:\n",
    "                current_chunk = segment\n",
    "                \n",
    "    if current_chunk:\n",
    "        final_chunks.append(current_chunk)\n",
    "\n",
    "    print(f\"切割完成，共得到 {len(final_chunks)} 个语义完整的文本块。\")\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c323613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_split_sentence(text, max_len):\n",
    "    \"\"\"辅助函数：按句子拆分超长段落\"\"\"\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    # 按优先级拆分：换行 > 句号 > 其他标点\n",
    "    splits = re.split(r'([\\n。！？；])', text) \n",
    "    \n",
    "    temp_sentence = \"\"\n",
    "    for part in splits:\n",
    "        temp_sentence += part\n",
    "        # 只有当遇到标点符号，或者攒够了一定长度时才检查\n",
    "        if len(temp_sentence) > max_len:\n",
    "             # 单句过长，强制按字切\n",
    "             chunks.append(temp_sentence[:max_len])\n",
    "             temp_sentence = temp_sentence[max_len:]\n",
    "        \n",
    "        if part in [\"\\n\", \"。\", \"！\", \"？\", \"；\"]: # 句子结束了\n",
    "            if len(current) + len(temp_sentence) > max_len:\n",
    "                if current: chunks.append(current)\n",
    "                current = temp_sentence\n",
    "            else:\n",
    "                current += temp_sentence\n",
    "            temp_sentence = \"\"\n",
    "            \n",
    "    # 处理剩余\n",
    "    if temp_sentence: current += temp_sentence\n",
    "    if current: chunks.append(current)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad50c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在调用 LibreOffice 加载文档...\n",
      "正在进行智能语义切割 (Max: 600, Min: 100)...\n",
      "已移除 '相关链接' 部分。\n",
      "切割完成，共得到 24 个语义完整的文本块。\n",
      "\n",
      "==============================\n",
      "      切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 101)\n",
      "内容: 〖特别报告〗      政府左右房价？（下）2021年06月02日福卡分析                       ...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 1 (长度: 3)\n",
      "内容: 摘要：...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 2 (长度: 475)\n",
      "内容: 摘要：文娱产业偏离文化的背后折射出时尚与经典文化的对冲，在人类文明转型阶段，文化上的变革是不可避免的，只不过，时尚化显然...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 3 (长度: 2)\n",
      "内容: 一，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 4 (长度: 149)\n",
      "内容: 一，明星IP化。明星效应膨胀式放大，并通过多个出口变现。最典型的便是拟上市公司和明星进行深度绑定，借其人气从资本市场获得...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 5 (长度: 2)\n",
      "内容: 二，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 6 (长度: 175)\n",
      "内容: 二，饭圈邪教化。“饭圈”粉丝构建了一套包括打投组、反黑组、安利站等在内的严密的组织体系，往往乐于将偶像标签化，如“美强惨...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 7 (长度: 2)\n",
      "内容: 三，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 8 (长度: 169)\n",
      "内容: 三，运作金融化。文娱产业俨然成为了一场裹挟着各路资本套路的“买卖”，如近期“倒奶事件”便是在“唯钱是举”的打投应援机制下...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 9 (长度: 2)\n",
      "内容: 四，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 10 (长度: 139)\n",
      "内容: 四，趣味低俗化。一切都能娱乐，一切都可以拿来娱乐，文化趣味也趋向以娱乐为标杆的审丑化、猎奇化、低俗化。各种小鲜肉网剧、打...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 17 (长度: 2)\n",
      "内容: 一，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 18 (长度: 138)\n",
      "内容: 一，现代与传统。经典文化观强调“历史照亮未来”，而时尚则是基于对传统行为方式、思维模式的破坏和建立，“反映了风俗习惯变迁...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 19 (长度: 2)\n",
      "内容: 二，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 20 (长度: 116)\n",
      "内容: 二，个性与群体性。经典文化观强调群体性的集成，而时尚尽管面向大众，但本质上崇拜个性的解放，如广告所标榜的是“穿出个性”，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 21 (长度: 2)\n",
      "内容: 三，...\n",
      "------------------------------\n",
      "\n",
      ">>> Chunk 22 (长度: 521)\n",
      "内容: 三，世俗与高雅。精英论者的文化观，认为形式“高雅”的才是文化，但时尚往往迎合的是大众“量大而浅层”的文化需求，这就难免世...\n",
      "------------------------------\n",
      "\n",
      "正在加载嵌入模型 'all-mpnet-base-v2'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在连接 Milvus...\n",
      "使用 Docker 模式 (192.168.16.138)...\n",
      "正在插入 24 条数据...\n",
      "正在写入磁盘 (Flush)...\n",
      "正在创建索引...\n",
      "\n",
      "任务完成！共存入 24 条纯净数据（已去除相关链接）。\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 主程序 ---\n",
    "def main():\n",
    "    full_text = load_doc_as_text(SOURCE_DOC_FILE)\n",
    "    if not full_text: return\n",
    "\n",
    "    # 执行切割\n",
    "    chunks = split_text_smart(full_text)\n",
    "    \n",
    "    # 预览检查\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"      切分效果预览      \")\n",
    "    print(\"=\"*30)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # 打印前几条，以及特定的标题块，方便检查\n",
    "        markers = [\"一，\", \"二，\", \"三，\", \"四，\", \"现代与传统\"]\n",
    "        is_marker_chunk = any(m in chunk[:20] for m in markers)\n",
    "        \n",
    "        if i < 3 or is_marker_chunk:\n",
    "            print(f\"\\n>>> Chunk {i} (长度: {len(chunk)})\")\n",
    "            clean_preview = chunk[:60].replace('\\n', ' ')\n",
    "            print(f\"内容: {clean_preview}...\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "    # 存入 Milvus\n",
    "    print(f\"\\n正在加载嵌入模型 '{MODEL_NAME}'...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    \n",
    "    print(\"正在连接 Milvus...\")\n",
    "    if USE_MILVUS_LITE:\n",
    "        print(\"使用 Milvus Lite 本地模式...\")\n",
    "        connections.connect(\"default\", uri=\"./milvus_demo.db\")\n",
    "    else:\n",
    "        print(f\"使用 Docker 模式 ({MILVUS_HOST})...\")\n",
    "        connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    \n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        \n",
    "    fields = [\n",
    "        FieldSchema(name=ID_FIELD, dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=VECTOR_FIELD, dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=TEXT_FIELD, dtype=DataType.VARCHAR, max_length=65535)\n",
    "    ]\n",
    "    collection = Collection(name=COLLECTION_NAME, schema=CollectionSchema(fields))\n",
    "    \n",
    "    print(f\"正在插入 {len(chunks)} 条数据...\")\n",
    "    collection.insert([embeddings, chunks])\n",
    "    \n",
    "    print(\"正在写入磁盘 (Flush)...\")\n",
    "    collection.flush() \n",
    "    \n",
    "    print(\"正在创建索引...\")\n",
    "    index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "    collection.create_index(VECTOR_FIELD, index_params)\n",
    "    collection.load()\n",
    "    \n",
    "    print(f\"\\n任务完成！共存入 {collection.num_entities} 条纯净数据（已去除相关链接）。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
