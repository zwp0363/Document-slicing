{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b1f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始测试：三级关键词匹配 + 前后句上下文 + AI 深度验证 ---\n",
      "\n",
      "提示：系统将自动提取 [前一句 + 当前句 + 后一句] 发送给大模型进行分析。\n",
      "\n",
      "[7] 文档: Global Climate Agreements: Successes and Failures...\n",
      "   Link: https://www.cfr.org/backgrounder/paris-global-climate-change-agreements\n",
      "   [中国相关]: 是\n",
      "   ★ 一级标签: 【战略资源】\n",
      "      ├── 二级标签: 【能源】\n",
      "      │     [新增三级标签匹配 & AI 分析]:\n",
      "      │       + 三级分类: <传统能源>\n",
      "      │           --------------------------------------------------\n",
      "      │           关键词: [Oil]\n",
      "      │           上下文: \"The United States and EU introduced aat COP26, which aims to slash 30 percent of methane emissions levels between 2020 and 2030. At COP28, oil companies announced they would cut their methane emissions from wells and drilling by more than 80 percent by the end of the decade. However, pledges to phase out fossil fuels were not renewed the following year at COP29.Most experts say that countries’ pledges are not ambitious enough and will not be enacted quickly enough to limit global temperature rise to 1.5°C.\"\n",
      "      │           ✅ AI 结论: 文本中提到的'Oil'属于传统能源的一种，与'传统能源'在语义上一致。\n",
      "      │           --------------------------------------------------\n",
      "      │           关键词: [Coal]\n",
      "      │           上下文: \"In recent years, climate diplomacy has occurred increasingly through minilateral groupings. The(G20), representing countries that are responsible for 80 percent of the world’s greenhouse gas pollution, has pledged to stop financing new coal-fired power plants abroad and agreed to triple renewable energy capacity by the end of this decade. However, G20 governments have thus far failed to set a deadline to phase out fossil fuels.\"\n",
      "      │           ✅ AI 结论: 根据上下文，'Coal' 明确指代传统能源中的煤炭资源。\n",
      "      │\n",
      "==========================================================================================\n",
      "[8] 文档: Myanmar’s Troubled History...\n",
      "   Link: https://www.cfr.org/backgrounder/myanmar-history-coup-military-rule-ethnic-conflict-rohingya\n",
      "   [中国相关]: 是\n",
      "   ★ 一级标签: 【战略资源】\n",
      "      ├── 二级标签: 【矿产】\n",
      "      │     [新增三级标签匹配 & AI 分析]:\n",
      "      │       + 三级分类: <金属矿产>\n",
      "      │           --------------------------------------------------\n",
      "      │           关键词: [Nickel]\n",
      "      │           上下文: \"Work on development projects has since resumed. But attacks have, with opposition forces disrupting a China-backed nickel plant in January, for example. At the same time, some of Myanmar’s top military leaders have, fearing that Nay Pyi Taw could fall too deeply into Beijing’s sphere of influence, according to the International Crisis Group.\"\n",
      "      │           ✅ AI 结论: 根据上下文提到的'nickel plant'，这里的'Nickel'明确指代金属矿产。\n",
      "      │\n",
      "      ├── 二级标签: 【能源】\n",
      "      │     [新增三级标签匹配 & AI 分析]:\n",
      "      │       + 三级分类: <传统能源>\n",
      "      │           --------------------------------------------------\n",
      "      │           关键词: [Oil]\n",
      "      │           上下文: \"China has funded infrastructure and energy projects throughout Myanmar as part of its. Oil and natural gas flow through pipelines from Myanmar to China. Beijing is also working to create ain Rakhine State to connect China’s landlocked Yunnan Province to the Indian Ocean.\"\n",
      "      │           ✅ AI 结论: 根据上下文提到的石油和天然气输送管道，'Oil' 明确指代传统能源。\n",
      "      │\n",
      "==========================================================================================\n",
      "[20] 文档: Trump says Xi approves of TikTok deal as leaders plan South ...\n",
      "   Link: https://www.bbc.com/news/articles/c4g7l7yl832o\n",
      "   [中国相关]: 是\n",
      "   ★ 一级标签: 【战略资源】\n",
      "      ├── 二级标签: 【矿产】\n",
      "      │     [仅原有关键词匹配]: ['minerals']\n",
      "      │\n",
      "==========================================================================================\n",
      "\n",
      "测试结束。\n",
      "共扫描文档: 20\n",
      "命中【战略资源】相关文档: 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import dashscope\n",
    "from http import HTTPStatus\n",
    "import spacy\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "\n",
    "# 1. 请在此处填入您的阿里千问 API Key\n",
    "dashscope.api_key = 'sk-fa13f585000140deabdfa506b25a7f3d' \n",
    "\n",
    "# 2. 数据库配置\n",
    "MONGO_URI = 'mongodb://admin:12345678@192.168.16.138:27017/?authSource=admin'\n",
    "DB_NAME = \"test\"\n",
    "COLLECTION_NAME = \"overseas_website_data_all\"\n",
    "\n",
    "# ===========================================\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "source_collection = db[COLLECTION_NAME]\n",
    "\n",
    "# --- 关键词配置 (保持不变) ---\n",
    "CHINA_KEYWORDS = [\n",
    "    \"China\", \"Chinese\", \"Beijing\", \"PRC\", \"People's Republic of China\", \"Mainland China\", \n",
    "    \"State Council\", \"Zhongnanhai\", \"CCP\", \"Communist Party of China\", \"Xi Jinping\", \n",
    "    \"Politburo\", \"Yuan\", \"RMB\", \"Renminbi\", \"Belt and Road\", \"BRI\", \"Made in China\", \n",
    "    \"Huawei\", \"Tencent\", \"Alibaba\", \"ByteDance\", \"TikTok\", \"PLA\", \"People's Liberation Army\", \n",
    "    \"Shanghai\", \"Shenzhen\", \"Hong Kong\", \"Macau\", \"Macao\", \"Taiwan\", \"Taipei\", \n",
    "    \"Xinjiang\", \"Tibet\", \"South China Sea\", \"Sino-\"\n",
    "]\n",
    "CHINA_KEYWORD_REGEX = [re.compile(r'\\bsino-', re.IGNORECASE)] + [\n",
    "    re.compile(r'\\b' + re.escape(kw.lower()) + r'\\b', re.IGNORECASE) for kw in CHINA_KEYWORDS if kw != \"Sino-\"\n",
    "]\n",
    "\n",
    "STRATEGIC_RESOURCES_CONFIG = {\n",
    "    \"稀土\": {\n",
    "        \"base_keywords\": [\"rare earth\", \"rare earth elements\", \"REE\"], \n",
    "        \"detailed_tags\": {\n",
    "            \"轻稀土\": [\"Light Rare Earth Elements\", \"LREE\", \"Lanthanum\", \"La\", \"Cerium\", \"Ce\", \"Praseodymium\", \"Pr\", \"Neodymium\", \"Nd\", \"Promethium\", \"Pm\", \"Samarium\", \"Sm\", \"Europium\", \"Eu\"],\n",
    "            \"重稀土\": [\"Heavy Rare Earth Elements\", \"HREE\", \"Gadolinium\", \"Gd\", \"Terbium\", \"Tb\", \"Dysprosium\", \"Dy\", \"Holmium\", \"Ho\", \"Erbium\", \"Er\", \"Thulium\", \"Tm\", \"Ytterbium\", \"Yb\", \"Lutetium\", \"Lu\", \"Yttrium\", \"Y\", \"Scandium\", \"Sc\"],\n",
    "            \"永磁材料\": [\"Permanent Magnets\", \"NdFeB\", \"Neodymium Iron Boron\", \"SmCo\", \"Samarium Cobalt\", \"rare earth magnet\"],\n",
    "            \"开采与分离技术\": [\"Mining Technology\", \"extraction technology\", \"Separation & Purification\", \"rare earth separation\", \"purification process\", \"solvent extraction\"],\n",
    "            \"回收利用\": [\"Recycling\", \"rare earth recycling\", \"urban mining\"]\n",
    "        }\n",
    "    },\n",
    "    \"矿产\": {\n",
    "        \"base_keywords\": [\"mineral\", \"minerals\", \"critical minerals\", \"strategic minerals\"],\n",
    "        \"detailed_tags\": {\n",
    "            \"能源矿产\": [\"Energy Minerals\", \"Uranium\", \"U\", \"Thorium\", \"Th\"],\n",
    "            \"金属矿产\": [\n",
    "                \"Metallic Minerals\", \"Lithium\", \"Li\", \"Cobalt\", \"Co\", \"Nickel\", \"Ni\", \n",
    "                \"Tungsten\", \"W\", \"Tin\", \"Sn\", \"Antimony\", \"Sb\", \"Beryllium\", \"Be\", \n",
    "                \"Niobium\", \"Nb\", \"Tantalum\", \"Ta\", \"Zirconium\", \"Zr\"\n",
    "            ],\n",
    "            \"非金属矿产\": [\"Non-metallic Minerals\", \"Fluorite\", \"Graphite\", \"Quartz\"],\n",
    "            \"采矿技术\": [\"Mining technology\", \"deep sea mining\", \"open-pit mining\"]\n",
    "        }\n",
    "    },\n",
    "    \"能源\": {\n",
    "        \"base_keywords\": [\"energy\", \"energy resources\"],\n",
    "        \"detailed_tags\": {\n",
    "            \"传统能源\": [\"Tradition Energy\", \"Petroleum\", \"Oil\", \"Coal\", \"Natural Gas\", \"LNG\", \"Fossil fuel\"],\n",
    "            \"新能源\": [\"New Energy\", \"Renewables\", \"Renewable energy\", \"Hydrogen Energy\", \"Solar Power\", \"Photovoltaic\", \"Nuclear Power\", \"Wind Power\", \"Energy Storage\", \"Battery storage\"],\n",
    "            \"能源运输\": [\"Energy Transport\", \"pipeline\", \"oil tanker\", \"LNG carrier\", \"energy grid\", \"transmission line\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_expanded_context(text, match_obj, max_chars=800):\n",
    "    \"\"\"\n",
    "    【核心逻辑升级】：提取 [前一句] + [当前句] + [后一句]\n",
    "    \"\"\"\n",
    "    keyword_start = match_obj.start()\n",
    "    keyword_end = match_obj.end()\n",
    "    text_len = len(text)\n",
    "    \n",
    "    # 定义标点符号集合 (用于判断句子边界)\n",
    "    punctuations = ['.', '?', '!']\n",
    "    \n",
    "    # --- 1. 寻找当前句子的边界 (Current Sentence) ---\n",
    "    # 向前找句首\n",
    "    curr_sent_start = 0\n",
    "    for i in range(keyword_start, -1, -1):\n",
    "        if text[i] in punctuations:\n",
    "            # 只有当标点后面有空格或换行时，才视为句子结束，避免 \"U.S.\" 误判\n",
    "            # 这里做一个简单处理：找到标点就停，位置+1为句首\n",
    "            curr_sent_start = i + 1\n",
    "            break\n",
    "            \n",
    "    # 向后找句尾\n",
    "    curr_sent_end = text_len\n",
    "    for i in range(keyword_end, text_len):\n",
    "        if text[i] in punctuations:\n",
    "            curr_sent_end = i + 1\n",
    "            break\n",
    "            \n",
    "    # --- 2. 寻找前一句的开始 (Previous Sentence) ---\n",
    "    # 从当前句首的前一个字符开始，继续向前找标点\n",
    "    prev_sent_start = 0 \n",
    "    if curr_sent_start > 0:\n",
    "        for i in range(curr_sent_start - 2, -1, -1):\n",
    "            if text[i] in punctuations:\n",
    "                prev_sent_start = i + 1\n",
    "                break\n",
    "    \n",
    "    # --- 3. 寻找后一句的结束 (Next Sentence) ---\n",
    "    # 从当前句尾的后一个字符开始，继续向后找标点\n",
    "    next_sent_end = text_len\n",
    "    if curr_sent_end < text_len:\n",
    "        for i in range(curr_sent_end + 1, text_len):\n",
    "            if text[i] in punctuations:\n",
    "                next_sent_end = i + 1\n",
    "                break\n",
    "                \n",
    "    # --- 4. 截取并清理 ---\n",
    "    # 截取范围：[前一句首 : 后一句尾]\n",
    "    expanded_text = text[prev_sent_start : next_sent_end].strip()\n",
    "    \n",
    "    # 安全限制：如果因为标点缺失导致提取内容过长，强制截断\n",
    "    if len(expanded_text) > max_chars:\n",
    "        # 回退到简单的窗口截取\n",
    "        start = max(0, keyword_start - 300)\n",
    "        end = min(text_len, keyword_end + 300)\n",
    "        expanded_text = \"...\" + text[start:end] + \"...\"\n",
    "        \n",
    "    # 清理多余换行，保持整洁\n",
    "    expanded_text = re.sub(r'\\s+', ' ', expanded_text)\n",
    "    \n",
    "    return expanded_text\n",
    "\n",
    "def verify_with_qwen(keyword, context, tag_name):\n",
    "    \"\"\"\n",
    "    Prompt 升级：明确告知大模型这是三句话的上下文\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    你是一个专业的战略资源情报分析师。\n",
    "    \n",
    "    【任务】\n",
    "    判断提供的文本片段中，特定的“关键词”是否在语义上属于“目标分类”。\n",
    "    \n",
    "    【输入信息】\n",
    "    1. 目标分类: \"{tag_name}\"\n",
    "    2. 待分析关键词: \"{keyword}\"\n",
    "    3. 文本片段 (包含关键词的前后句上下文): \n",
    "    \"{context}\"\n",
    "\n",
    "    【判断逻辑】\n",
    "    请阅读整个文本片段，分析该关键词的含义。\n",
    "    - 如果关键词明确指代\"{tag_name}\"（例如 'Li' 指代锂元素，'Co' 指代钴矿），返回 true。\n",
    "    - 如果关键词是缩写、人名的一部分、或其他普通单词（例如 'Co' 是 Co-operation, 'W' 是 George W. Bush），返回 false。\n",
    "    - 如果语境完全无关，返回 false。\n",
    "\n",
    "    【输出格式】\n",
    "    请仅返回标准 JSON 字符串：\n",
    "    {{\n",
    "        \"is_match\": true,\n",
    "        \"reason\": \"请用一句话中文解释，例如：根据后文提到的电池生产，这里的Li指代锂资源。\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = dashscope.Generation.call(\n",
    "            model='qwen-turbo', # 使用 turbo 即可，速度快\n",
    "            prompt=prompt,\n",
    "            result_format='message',\n",
    "        )\n",
    "\n",
    "        if response.status_code == HTTPStatus.OK:\n",
    "            content = response.output.choices[0].message.content\n",
    "            # 清理 Markdown 代码块标记\n",
    "            content = content.replace('```json', '').replace('```', '').strip()\n",
    "            return content\n",
    "        else:\n",
    "            return f'{{\"is_match\": false, \"reason\": \"API Error: {response.code}\"}}'\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f'{{\"is_match\": false, \"reason\": \"Exception: {str(e)}\"}}'\n",
    "\n",
    "# 修改\n",
    "# 加载轻量级英语模型\n",
    "# 确保你已经安装并下载了模型: python -m spacy download en_core_web_sm\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"正在下载 spacy 模型...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def check_keywords_with_spacy(text, keywords):\n",
    "    \"\"\"\n",
    "    使用 spaCy 进行 NLP 过滤的关键词匹配\n",
    "    \"\"\"\n",
    "    found_items = []\n",
    "    seen_kws = set()\n",
    "    \n",
    "    # 将文本转为 spaCy 文档对象\n",
    "    # 增加 text[:1000000] 限制防止超大文本爆内存，视情况调整\n",
    "    doc = nlp(text[:1000000]) \n",
    "    \n",
    "    # 构建关键词映射\n",
    "    target_kws = {kw.lower(): kw for kw in keywords}\n",
    "    \n",
    "    for token in doc:\n",
    "        token_text_lower = token.text.lower()\n",
    "        \n",
    "        # 1. 基础匹配\n",
    "        if token_text_lower not in target_kws:\n",
    "            continue\n",
    "            \n",
    "        original_kw = target_kws[token_text_lower]\n",
    "        \n",
    "        # --- 2. NLP 智能过滤规则 ---\n",
    "        \n",
    "        # 规则 A: 排除人名 (PERSON)\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            continue\n",
    "            \n",
    "        # 规则 B: 排除组织/国家缩写 (GPE, ORG)\n",
    "        if token.ent_type_ in [\"GPE\", \"ORG\"]:\n",
    "            continue\n",
    "            \n",
    "        # 规则 C: 排除动词和常用虚词\n",
    "        if token.pos_ in [\"VERB\", \"DET\", \"ADP\", \"PRON\", \"AUX\"]:\n",
    "            continue\n",
    "\n",
    "        # 规则 D: 特殊符号清洗 (排除 U.S. 这种情况)\n",
    "        # 检查当前 token 的下一个 token 是否是 \".\"\n",
    "        if token.i + 1 < len(doc) and doc[token.i + 1].text == '.':\n",
    "            continue\n",
    "            \n",
    "        # 规则 E: 大小写敏感校验 (针对短词)\n",
    "        if len(original_kw) <= 2:\n",
    "            if not token.text[0].isupper():\n",
    "                continue\n",
    "        \n",
    "        # --- 3. 提取上下文 (逻辑修复) ---\n",
    "        if original_kw.lower() in seen_kws:\n",
    "            continue\n",
    "            \n",
    "        # 获取当前句子 Span\n",
    "        current_sent = token.sent\n",
    "        \n",
    "        # 获取前一句\n",
    "        # 方法：找到当前句子第一个Token的前一个Token，取其所在的句子\n",
    "        prev_sent_text = \"\"\n",
    "        if current_sent.start > 0:\n",
    "            prev_token_index = current_sent.start - 1\n",
    "            prev_sent_text = doc[prev_token_index].sent.text\n",
    "\n",
    "        # 获取后一句\n",
    "        # 方法：找到当前句子最后一个Token的后一个Token，取其所在的句子\n",
    "        next_sent_text = \"\"\n",
    "        if current_sent.end < len(doc):\n",
    "            next_token_index = current_sent.end\n",
    "            next_sent_text = doc[next_token_index].sent.text\n",
    "            \n",
    "        # 拼接上下文\n",
    "        context = f\"{prev_sent_text} {current_sent.text} {next_sent_text}\".strip()\n",
    "        context = re.sub(r'\\s+', ' ', context)\n",
    "        \n",
    "        found_items.append((original_kw, context))\n",
    "        seen_kws.add(original_kw.lower())\n",
    "            \n",
    "    return found_items\n",
    "\n",
    "def test_strategic_resource_tagging():\n",
    "    print(\"--- 开始测试：三级关键词匹配 + 前后句上下文 + AI 深度验证 ---\\n\")\n",
    "    print(\"提示：系统将自动提取 [前一句 + 当前句 + 后一句] 发送给大模型进行分析。\\n\")\n",
    "    \n",
    "    cursor = source_collection.find({}, {\"title\": 1, \"content\": 1, \"link\": 1}).limit(20)\n",
    "    \n",
    "    total_processed = 0\n",
    "    matched_docs = 0\n",
    "    \n",
    "    for doc in cursor:\n",
    "        total_processed += 1\n",
    "        content = doc.get(\"content\", \"\")\n",
    "        title = doc.get(\"title\", \"\")\n",
    "        link = doc.get(\"link\", \"\")\n",
    "        \n",
    "        full_text = (str(title) + \". \" + str(content)).strip()\n",
    "        if not full_text:\n",
    "            continue\n",
    "\n",
    "        is_china_related = False\n",
    "        for regex in CHINA_KEYWORD_REGEX:\n",
    "            if regex.search(full_text.lower()):\n",
    "                is_china_related = True\n",
    "                break\n",
    "        \n",
    "        doc_matches = {}\n",
    "        has_match_in_doc = False\n",
    "\n",
    "        for level_2_tag, config in STRATEGIC_RESOURCES_CONFIG.items():\n",
    "            # 基础匹配\n",
    "            base_hits_tuples = check_keywords_with_spacy(full_text, config[\"base_keywords\"])\n",
    "            \n",
    "            # 详细三级匹配\n",
    "            detailed_hits_info = {}\n",
    "            has_detailed = False\n",
    "            \n",
    "            for level_3_tag, sub_keywords in config[\"detailed_tags\"].items():\n",
    "                hits_tuples = check_keywords_with_spacy(full_text, sub_keywords)\n",
    "                if hits_tuples:\n",
    "                    detailed_hits_info[level_3_tag] = hits_tuples\n",
    "                    has_detailed = True\n",
    "            \n",
    "            if base_hits_tuples or has_detailed:\n",
    "                doc_matches[level_2_tag] = {\n",
    "                    \"base\": base_hits_tuples,\n",
    "                    \"detailed\": detailed_hits_info\n",
    "                }\n",
    "                has_match_in_doc = True\n",
    "        \n",
    "        if has_match_in_doc:\n",
    "            matched_docs += 1\n",
    "            print(f\"[{total_processed}] 文档: {title[:60]}...\")\n",
    "            print(f\"   Link: {link}\")\n",
    "            print(f\"   [中国相关]: {'是' if is_china_related else '否'}\")\n",
    "            print(f\"   ★ 一级标签: 【战略资源】\") \n",
    "\n",
    "            for level_2, data in doc_matches.items():\n",
    "                base_tuples = data['base']\n",
    "                detailed_dict = data['detailed']\n",
    "                \n",
    "                print(f\"      ├── 二级标签: 【{level_2}】\")\n",
    "                \n",
    "                # 打印旧匹配结果 (略)\n",
    "                if not detailed_dict and not base_tuples:\n",
    "                    print(\"      │     (无匹配)\")\n",
    "                elif base_tuples and not detailed_dict:\n",
    "                    print(f\"      │     [仅原有关键词匹配]: { [x[0] for x in base_tuples] }\")\n",
    "\n",
    "                # 重点打印三级匹配及分析\n",
    "                if detailed_dict:\n",
    "                    print(f\"      │     [新增三级标签匹配 & AI 分析]:\")\n",
    "                    for level_3, hits in detailed_dict.items():\n",
    "                        print(f\"      │       + 三级分类: <{level_3}>\")\n",
    "                        for kw, ctx in hits:\n",
    "                            print(f\"      │           --------------------------------------------------\")\n",
    "                            print(f\"      │           关键词: [{kw}]\")\n",
    "                            print(f\"      │           上下文: \\\"{ctx}\\\"\")\n",
    "                            \n",
    "                            # 调用 AI\n",
    "                            ai_result_json = verify_with_qwen(kw, ctx, level_3)\n",
    "                            \n",
    "                            try:\n",
    "                                res = json.loads(ai_result_json)\n",
    "                                is_match = res.get(\"is_match\")\n",
    "                                reason = res.get(\"reason\")\n",
    "                                icon = \"✅\" if is_match else \"❌\"\n",
    "                                print(f\"      │           {icon} AI 结论: {reason}\")\n",
    "                            except:\n",
    "                                print(f\"      │           ⚠️ AI JSON 解析失败: {ai_result_json}\")\n",
    "                            \n",
    "                            time.sleep(0.3) \n",
    "                \n",
    "                print(\"      │\")\n",
    "            print(\"=\" * 90)\n",
    "\n",
    "    print(f\"\\n测试结束。\")\n",
    "    print(f\"共扫描文档: {total_processed}\")\n",
    "    print(f\"命中【战略资源】相关文档: {matched_docs}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        test_strategic_resource_tagging()\n",
    "    except Exception as e:\n",
    "        print(f\"程序运行出错: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
