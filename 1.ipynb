{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916e6d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档加载成功，长度: 5969 字符\n",
      "文档预览: \n",
      "\n",
      "\n",
      "\n",
      "〖特别报告〗      政府左右房价？（下）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2021年06月02日福卡分...\n",
      "正在使用通用策略切割文本 (目标大小: 500 字符)...\n",
      "切割完成，共得到 13 个文本块。\n",
      "\n",
      "--- 块预览 ---\n",
      "Chunk 0: 〖特别报告〗      政府左右房价？（下）2021年06月02日福卡分析             ...\n",
      "Chunk 1: 2020年全国规模以上文化及相关产业企业实现营业收入98514亿元，其中，互联网+其他信息服务、其他...\n",
      "Chunk 2: “你我本无缘，全靠我花钱”也直接催生了诸多追星金融套路，最具争议的莫过于集资，据蓝鲸财经报道，截至4...\n",
      "\n",
      "正在加载嵌入模型 'all-mpnet-base-v2'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在连接 Milvus (192.168.16.138)...\n",
      "正在向 Milvus 插入 13 条数据...\n",
      "正在等待数据写入磁盘 (Flush)...\n",
      "正在创建索引...\n",
      "\n",
      "成功存入 13 条数据到 Milvus！\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "# --- 1. 配置 ---\n",
    "SOURCE_DOC_FILE = \"./yq2021-0602文娱产业正离文化越来越远.doc\"\n",
    "\n",
    "# Milvus 配置\n",
    "MILVUS_HOST = \"192.168.16.138\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"report_analysis\"\n",
    "ID_FIELD = \"chunk_id\"\n",
    "VECTOR_FIELD = \"vector\"\n",
    "TEXT_FIELD = \"text_content\"\n",
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# --- 2. 文档加载函数 (保持不变) ---\n",
    "def find_libreoffice():\n",
    "    possible_paths = [\n",
    "        r\"C:\\Program Files\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"C:\\Program Files (x86)\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"D:\\LibreOffice\\program\\soffice.exe\",\n",
    "        \"soffice\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path != \"soffice\" and not os.path.exists(path): continue\n",
    "            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0: return path\n",
    "        except: continue\n",
    "    return None\n",
    "\n",
    "def load_doc_as_text(doc_path):\n",
    "    # ... (与之前的加载逻辑相同，为节省空间省略重复代码，核心是转HTML提取文本) ...\n",
    "    # 这里为了完整性，简写核心逻辑：\n",
    "    libreoffice_path = find_libreoffice()\n",
    "    if not libreoffice_path: return None\n",
    "    \n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    output_dir = os.path.dirname(doc_path)\n",
    "    html_filename = os.path.basename(doc_path).rsplit('.', 1)[0] + '.html'\n",
    "    html_path = os.path.join(output_dir, html_filename)\n",
    "    \n",
    "    if os.path.exists(html_path): os.remove(html_path)\n",
    "    \n",
    "    cmd = [libreoffice_path, '--headless', '--convert-to', 'html', '--outdir', output_dir, doc_path]\n",
    "    subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if not os.path.exists(html_path): return None\n",
    "    \n",
    "    # 尝试读取 (优先utf-8, 其次gb18030)\n",
    "    content = \"\"\n",
    "    for enc in ['utf-8', 'gb18030']:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding=enc) as f:\n",
    "                content = f.read()\n",
    "                break\n",
    "        except: continue\n",
    "            \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # 清理临时文件\n",
    "    try: os.remove(html_path)\n",
    "    except: pass\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- 3. 新的通用切割函数 (核心修改) ---\n",
    "def split_text_recursive(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    通用递归切割：适用于任何文档。\n",
    "    不依赖特定标题，而是利用标点符号和段落进行智能分割。\n",
    "    \"\"\"\n",
    "    print(f\"正在使用通用策略切割文本 (目标大小: {chunk_size} 字符)...\")\n",
    "    \n",
    "    # 1. 基础清理\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", text) # 去除控制字符\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text) # 合并空行\n",
    "    \n",
    "    # 2. 定义分隔符优先级 (从大到小)\n",
    "    # 先按段落切，太长就按句号切，还太长就按逗号切\n",
    "    separators = [\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \"；\", \"，\", \" \"]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # 简单的递归切割实现 (无需安装 LangChain)\n",
    "    def recursive_split(text, separators):\n",
    "        final_chunks = []\n",
    "        \n",
    "        # 找到当前优先级最高的分隔符\n",
    "        separator = separators[0]\n",
    "        next_separators = separators[1:] if len(separators) > 1 else []\n",
    "        \n",
    "        # 如果没有分隔符了，或者文本已经够短，直接返回\n",
    "        if not separator:\n",
    "            return [text]\n",
    "        \n",
    "        # 按分隔符切割\n",
    "        splits = text.split(separator)\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for split in splits:\n",
    "            # 恢复分隔符 (除了空格和换行，其他的标点最好保留)\n",
    "            if separator not in [\"\\n\", \"\\n\\n\", \" \"]:\n",
    "                split += separator\n",
    "                \n",
    "            # 如果当前块加上这一段还很小，就合并\n",
    "            if len(current_chunk) + len(split) < chunk_size:\n",
    "                current_chunk += split\n",
    "            else:\n",
    "                # 如果当前块已经有内容，先保存\n",
    "                if current_chunk:\n",
    "                    final_chunks.append(current_chunk)\n",
    "                    current_chunk = \"\"\n",
    "                \n",
    "                # 如果新的一段本身就很大，需要递归继续切\n",
    "                if len(split) > chunk_size and next_separators:\n",
    "                    sub_chunks = recursive_split(split, next_separators)\n",
    "                    final_chunks.extend(sub_chunks)\n",
    "                else:\n",
    "                    # 否则直接作为新的一块\n",
    "                    current_chunk = split\n",
    "        \n",
    "        # 处理最后剩余的块\n",
    "        if current_chunk:\n",
    "            final_chunks.append(current_chunk)\n",
    "            \n",
    "        return final_chunks\n",
    "\n",
    "    chunks = recursive_split(text, separators)\n",
    "    \n",
    "    # 过滤太短的块\n",
    "    chunks = [c.strip() for c in chunks if len(c.strip()) > 20]\n",
    "    \n",
    "    print(f\"切割完成，共得到 {len(chunks)} 个文本块。\")\n",
    "    return chunks\n",
    "\n",
    "# --- 4. 主流程 ---\n",
    "def main():\n",
    "    # 1. 加载\n",
    "    full_text = load_doc_as_text(SOURCE_DOC_FILE)\n",
    "    if not full_text: \n",
    "        print(\"无法读取文件内容。\")\n",
    "        return\n",
    "\n",
    "    print(f\"文档加载成功，长度: {len(full_text)} 字符\")\n",
    "    print(f\"文档预览: {full_text[:50]}...\")\n",
    "\n",
    "    # 2. 切割 (使用新的通用函数)\n",
    "    chunks = split_text_recursive(full_text)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"切割结果为空，请检查文档内容。\")\n",
    "        return\n",
    "\n",
    "    # 预览\n",
    "    print(\"\\n--- 块预览 ---\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"Chunk {i}: {chunk[:50]}...\")\n",
    "\n",
    "    # 3. 存入 Milvus (保持原有逻辑)\n",
    "    print(f\"\\n正在加载嵌入模型 '{MODEL_NAME}'...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"正在连接 Milvus ({MILVUS_HOST})...\")\n",
    "    # 如果是本地文件模式请修改这里: connections.connect(\"default\", uri=\"./milvus_demo.db\")\n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    \n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        \n",
    "    fields = [\n",
    "        FieldSchema(name=ID_FIELD, dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=VECTOR_FIELD, dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=TEXT_FIELD, dtype=DataType.VARCHAR, max_length=65535)\n",
    "    ]\n",
    "    collection = Collection(name=COLLECTION_NAME, schema=CollectionSchema(fields))\n",
    "    \n",
    "    #  插入数据\n",
    "    print(f\"正在向 Milvus 插入 {len(chunks)} 条数据...\")\n",
    "    collection.insert([embeddings, chunks])\n",
    "    \n",
    "    print(\"正在等待数据写入磁盘 (Flush)...\")\n",
    "    collection.flush() \n",
    "\n",
    "    #  创建索引\n",
    "    print(\"正在创建索引...\")\n",
    "    collection.create_index(\n",
    "        VECTOR_FIELD, \n",
    "        {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "    )\n",
    "    \n",
    "    #  加载并显示结果\n",
    "    collection.load()\n",
    "    \n",
    "    print(f\"\\n成功存入 {collection.num_entities} 条数据到 Milvus！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
