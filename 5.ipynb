{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da717f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ac06e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置 ---\n",
    "SOURCE_DOC_FILE = \"./yq2021-0112化解当下世界经济衰退的根本之道——基础设施建设.doc\"\n",
    "\n",
    "# Milvus 配置\n",
    "USE_MILVUS_LITE = False  # 您正在使用 Docker 模式\n",
    "MILVUS_HOST = \"192.168.16.138\" \n",
    "MILVUS_PORT = \"19530\"\n",
    "\n",
    "COLLECTION_NAME = \"report_analysis\"\n",
    "ID_FIELD = \"chunk_id\"\n",
    "VECTOR_FIELD = \"vector\"\n",
    "TEXT_FIELD = \"text_content\"\n",
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "MAX_CHUNK_LEN = 500  # 合并的目标上限\n",
    "MIN_CHUNK_LEN = 100  # 合并的最小阈值\n",
    "HARD_MAX_LEN = 1000 # 【新】单个段落的硬性拆分上限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98f77b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 文档加载 ---\n",
    "def find_libreoffice():\n",
    "    possible_paths = [\n",
    "        r\"C:\\Program Files\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"C:\\Program Files (x86)\\LibreOffice\\program\\soffice.exe\",\n",
    "        r\"D:\\LibreOffice\\program\\soffice.exe\",\n",
    "        \"soffice\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path != \"soffice\" and not os.path.exists(path): continue\n",
    "            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0: return path\n",
    "        except: continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc_as_text(doc_path):\n",
    "    print(\"正在加载文档...\")\n",
    "    libreoffice_path = find_libreoffice()\n",
    "    if not libreoffice_path:\n",
    "        print(\"错误：未找到 LibreOffice (soffice.exe)，请检查路径。\")\n",
    "        return None\n",
    "    \n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    output_dir = os.path.dirname(doc_path)\n",
    "    html_filename = os.path.basename(doc_path).rsplit('.', 1)[0] + '.html'\n",
    "    html_path = os.path.join(output_dir, html_filename)\n",
    "    \n",
    "    if os.path.exists(html_path): \n",
    "        try: os.remove(html_path)\n",
    "        except: pass\n",
    "        \n",
    "    cmd = [libreoffice_path, '--headless', '--convert-to', 'html', '--outdir', output_dir, doc_path]\n",
    "    subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if not os.path.exists(html_path):\n",
    "        print(f\"错误：LibreOffice 转换失败，未在 {output_dir} 找到 {html_filename}\")\n",
    "        return None\n",
    "    \n",
    "    content = \"\"\n",
    "    for enc in ['utf-8', 'gb18030', 'gbk']:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding=enc) as f:\n",
    "                content = f.read()\n",
    "                break\n",
    "        except: continue\n",
    "            \n",
    "    if not content:\n",
    "        print(\"错误：读取转换后的 HTML 文件失败。\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # --- 移除页眉和页脚 ---\n",
    "    # 在查找 <p> 标签之前，先把页眉和页脚的 div 整个移除掉\n",
    "    print(\"   -> 正在移除页眉 (header) 和页脚 (footer)...\")\n",
    "    \n",
    "    # 根据您提供的 .html 源码，页眉和页脚是 <div title=\"header\"> 和 <div title=\"footer\">\n",
    "    header = soup.find('div', title='header')\n",
    "    if header:\n",
    "        header.decompose() # 移除页眉\n",
    "\n",
    "    footer = soup.find('div', title='footer')\n",
    "    if footer:\n",
    "        footer.decompose() # 移除页脚\n",
    "    \n",
    "    # 1. 查找所有 *剩余的* <p> 标签\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    cleaned_paragraphs = []\n",
    "    for p in paragraphs:\n",
    "        # 2. 对每个 <p> 标签，获取其所有内部文本\n",
    "        p_text = p.get_text().strip()\n",
    "        \n",
    "        # 3. 只有非空的段落才保留\n",
    "        if p_text:\n",
    "            cleaned_paragraphs.append(p_text)\n",
    "    \n",
    "    # 4. 用单一的、可靠的 \\n 将所有“干净”的段落连接起来\n",
    "    text = \"\\n\".join(cleaned_paragraphs)\n",
    "    \n",
    "    try: \n",
    "        os.remove(html_path) # 恢复自动删除\n",
    "        print(\"   -> 中间 HTML 文件已清理。\")\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 语义切割逻辑 (新 v3 - 增加硬上限) ---\n",
    "import re\n",
    "\n",
    "def split_long_paragraph(text, hard_max):\n",
    "    \"\"\"\n",
    "    【新规则】: 将 > hard_max (1000) 的单个段落从“中间的句号”切分为两块。\n",
    "    \"\"\"\n",
    "    print(f\"   -> 检测到长段落 (长度 {len(text)})，尝试按中间句号 '。' 切分...\")\n",
    "    mid_point = len(text) // 2\n",
    "    \n",
    "    # 1. 寻找离中点最近的句号\n",
    "    pos_before = text.rfind('。', 0, mid_point)\n",
    "    pos_after = text.find('。', mid_point)\n",
    "    \n",
    "    split_pos = -1\n",
    "    \n",
    "    if pos_before != -1 and pos_after != -1:\n",
    "        # 找到前后都有，取最近的\n",
    "        if (mid_point - pos_before) < (pos_after - mid_point):\n",
    "            split_pos = pos_before\n",
    "        else:\n",
    "            split_pos = pos_after\n",
    "    elif pos_before != -1:\n",
    "        # 只有前面有\n",
    "        split_pos = pos_before\n",
    "    elif pos_after != -1:\n",
    "        # 只有后面有\n",
    "        split_pos = pos_after\n",
    "    else:\n",
    "        # 整个段落没有句号\n",
    "        print(f\"   -> 警告: 段落长度 {len(text)} > {hard_max}，但未找到 '。' 无法切分。\")\n",
    "        return [text] # 无法切分，返回原样\n",
    "    \n",
    "    # 2. 执行切分 (句号保留在前一块)\n",
    "    part1 = text[:split_pos + 1].strip()\n",
    "    part2 = text[split_pos + 1:].strip()\n",
    "    \n",
    "    if not part1 or not part2:\n",
    "         print(f\"   -> 警告: 按句号切分失败 (产生空块)，返回原长段落。\")\n",
    "         return [text] # 避免切出空块\n",
    "         \n",
    "    print(f\"   -> 成功切分为两块: (长度 {len(part1)}) 和 (长度 {len(part2)})\")\n",
    "    return [part1, part2]\n",
    "\n",
    "\n",
    "def split_text_smart(text, max_len=MAX_CHUNK_LEN, min_len=MIN_CHUNK_LEN):\n",
    "    \"\"\"\n",
    "    【新逻辑 v3】：\n",
    "    1. 按段落提取。\n",
    "    2. > 1000字的长段落按中间句号切分。\n",
    "    3. 其余段落按 500 字上限合并。\n",
    "    4. < 100字的小块与后续块合并。\n",
    "    \"\"\"\n",
    "    print(f\"正在按段落切割 (合并目标: {max_len}, 最小: {min_len}, 硬上限: {HARD_MAX_LEN})...\")\n",
    "    \n",
    "    # 1. 移除废弃内容\n",
    "    text = re.split(r\"〖相关链接：信息〗|〖相关链接：报告〗\", text, 1)[0]\n",
    "    \n",
    "    # 2. 基础清理 (保留了 v2 的修复)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1f\\x7f-\\x9f]\", \"\", text)\n",
    "    text = re.sub(r\"[\\r\\n\\t ]+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text).strip()\n",
    "    \n",
    "    paragraphs = text.split('\\n')\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "    # 3. 长度控制\n",
    "    initial_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        \n",
    "        # 3a. 检查单个段落是否超过硬上限 (1000)\n",
    "        if len(para) > HARD_MAX_LEN:\n",
    "            sub_paragraphs = split_long_paragraph(para, HARD_MAX_LEN)\n",
    "        else:\n",
    "            sub_paragraphs = [para]\n",
    "\n",
    "        # 3b. 迭代处理(可能被切分后)的段落，并应用合并逻辑\n",
    "        for sub_para in sub_paragraphs:\n",
    "            if not sub_para.strip(): continue # 确保子段落不是空的\n",
    "            \n",
    "            # (应用 500 字的合并逻辑)\n",
    "            if not current_chunk:\n",
    "                current_chunk = sub_para\n",
    "            elif len(current_chunk) + len(sub_para) + 1 <= max_len: \n",
    "                current_chunk += \"\\n\" + sub_para\n",
    "            else:\n",
    "                initial_chunks.append(current_chunk)\n",
    "                current_chunk = sub_para\n",
    "    \n",
    "    # 保存最后一个累积的块\n",
    "    if current_chunk:\n",
    "        initial_chunks.append(current_chunk)\n",
    "        \n",
    "    print(f\"初步切割得到 {len(initial_chunks)} 个块，正在进行小块合并...\")\n",
    "\n",
    "    # 4. 后处理：合并小于 min_len (100) 的块\n",
    "    final_chunks = []\n",
    "    i = 0\n",
    "    while i < len(initial_chunks):\n",
    "        current_chunk = initial_chunks[i]\n",
    "        \n",
    "        if len(current_chunk) < min_len and (i < len(initial_chunks) - 1):\n",
    "            next_chunk = initial_chunks[i+1]\n",
    "            merged_chunk = current_chunk + \"\\n\" + next_chunk\n",
    "            \n",
    "            if len(merged_chunk) <= max_len:\n",
    "                final_chunks.append(merged_chunk)\n",
    "                i += 2 \n",
    "            else:\n",
    "                final_chunks.append(current_chunk)\n",
    "                i += 1\n",
    "        else:\n",
    "            final_chunks.append(current_chunk)\n",
    "            i += 1\n",
    "\n",
    "    print(f\"清理完成，最终得到 {len(final_chunks)} 个文本块。\")\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "057ca037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载文档...\n",
      "   -> 正在移除页眉 (header) 和页脚 (footer)...\n",
      "   -> 中间 HTML 文件已清理。\n",
      "正在按段落切割 (合并目标: 500, 最小: 100, 硬上限: 1000)...\n",
      "初步切割得到 9 个块，正在进行小块合并...\n",
      "清理完成，最终得到 9 个文本块。\n",
      "\n",
      "==============================\n",
      "      最终切分效果预览      \n",
      "==============================\n",
      "\n",
      ">>> Chunk 0 (长度: 106)\n",
      "内容: 〖特别报告〗 化解当下世界经济衰退的根本之道 ——基础设施建设 摘要：人类历史遭逢二战以来最为严重的经济衰退俨然成为定局，冲破至暗时刻化解全球经济衰退或将倚仗中...\n",
      "\n",
      ">>> Chunk 1 (长度: 569)\n",
      "内容: “此次经济衰退的范围和速度尚无现代先例，较之以往任何衰退都要严重得多。我们看到经济活动和就业率急剧下降，过去十年的经济成就已被倏然抹去……”美联储主席鲍威尔此番...\n",
      "\n",
      ">>> Chunk 2 (长度: 459)\n",
      "内容: 古语云“以史为鉴，方能知兴替”，人类经济史并不乏在遭遇经济萧条时冲破至暗时刻的先例，面对此轮史无前例的大衰退，亦可从历史发展的脉络中寻觅破局之道——基础设施建设...\n",
      "\n",
      ">>> Chunk 3 (长度: 427)\n",
      "内容: 二，日本基建狂潮成为经济托底神器。早在上世纪60年代，日本前首相田中角荣便提出“日本列岛改造论”，力主通过大规模建设全国新的铁路干线和高速公路来整顿交通网络，借...\n",
      "\n",
      ">>> Chunk 4 (长度: 612)\n",
      "内容: 三，“基建狂魔”创造中国式经济奇迹。就基建而言，中国可谓“青出于蓝”，是通过“大基建”让经济涅槃重生的历史践行者。1998年亚洲金融危机，中国经济遭受重创，出口...\n",
      "\n",
      "... (及其他 4 个块)\n",
      "\n",
      "正在生成向量...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在连接 Milvus (192.168.16.138)...\n",
      "正在写入磁盘 (Flush)...\n",
      "\n",
      "✅ 任务完成！共存入 9 条数据。\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 主程序 ---\n",
    "def main():\n",
    "    full_text = load_doc_as_text(SOURCE_DOC_FILE)\n",
    "    if not full_text: \n",
    "        print(\"错误：未能加载文档。\")\n",
    "        return\n",
    "\n",
    "    # 1. 执行切割\n",
    "    chunks = split_text_smart(full_text)\n",
    "    \n",
    "    # 2. 预览检查\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"      最终切分效果预览      \")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # 【修改点】简化预览逻辑，只看前 5 块和总块数\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < 5: # 只预览前5个\n",
    "            print(f\"\\n>>> Chunk {i} (长度: {len(chunk)})\")\n",
    "            preview = chunk.replace(\"\\n\", \" \")[:80]\n",
    "            print(f\"内容: {preview}...\")\n",
    "            \n",
    "            # 再次校验\n",
    "            if len(chunk) < MIN_CHUNK_LEN and i < len(chunks) - 1:\n",
    "                # 这个警告是正常的，如果它无法与前后合并\n",
    "                print(f\"   -> 提示：块 {i} 长度为 {len(chunk)}，小于 {MIN_CHUNK_LEN}。\")\n",
    "    \n",
    "    if len(chunks) > 5:\n",
    "        print(f\"\\n... (及其他 {len(chunks) - 5} 个块)\")\n",
    "\n",
    "    # 3. 存入 Milvus\n",
    "    print(f\"\\n正在生成向量...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"正在连接 Milvus ({MILVUS_HOST})...\")\n",
    "    if USE_MILVUS_LITE:\n",
    "        connections.connect(\"default\", uri=\"./milvus_demo.db\")\n",
    "    else:\n",
    "        connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    \n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        \n",
    "    fields = [\n",
    "        FieldSchema(name=ID_FIELD, dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=VECTOR_FIELD, dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=TEXT_FIELD, dtype=DataType.VARCHAR, max_length=65535)\n",
    "    ]\n",
    "    collection = Collection(name=COLLECTION_NAME, schema=CollectionSchema(fields))\n",
    "    \n",
    "    collection.insert([embeddings, chunks])\n",
    "    print(\"正在写入磁盘 (Flush)...\")\n",
    "    collection.flush() \n",
    "    \n",
    "    index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "    collection.create_index(VECTOR_FIELD, index_params)\n",
    "    collection.load()\n",
    "    \n",
    "    print(f\"\\n✅ 任务完成！共存入 {collection.num_entities} 条数据。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
